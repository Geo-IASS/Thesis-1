%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Introduction}
\label{commuteTime:introduction}
Commute time is a robust distance metric derived from a random walk on graphs
\cite{Khoa:2012}. In \citetitle{Khoa:2012}, \citeauthor{Khoa:2012} demonstrated
how commute time can be used as a distance measure for data mining tasks such as
anomaly detection and clustering. A prohibitive limitation of this technique is
that the calculation of commute time involves the Eigen decomposition of the
graph Laplacian, making it impractical for large graphs.

Specifically, commute time is an Euclidean distance in the space spanned by
eigenvectors of the graph Laplacian matrix. Commute time forms an interesting
and promising basis for an anomaly detection algorithm because, unlike
traditional Euclidean distance, the commute time between two nodes can capture
both the distance between them and the data densities so that we can capture
both global and local anomalies in the data set \cite{Khoa:2012}.

The commute time between two nodes $i$ and $j$ in a graph is the number of steps
that a random walk, starting from $i$ will take to visit $j$ and then come back
to $i$ for the first time. The fact that the commute time is averaged over all
paths (and not just the shortest path) makes it more robust to data
perturbations and it can also capture graph density \cite{Khoa:2012}. Since it
is a measure which can capture the geometrical structure of the data and is
robust to noise, commute time can be applied in methods where Euclidean or other
distances are used and thus the limitations of these metrics can be avoided.

\begin{figure}
    \centering
    \includegraphics[width=0.3\textwidth]{commute-time/example}
    \caption[A simple example illustrating the application of commute time]{A
        simple example illustrating the application of commute time
        \cite{Khoa:2012}}
    \label{fig:commuteTime:example}
\end{figure}

A simple example illustrating the application of the commute time metric is
shown in \autoref{fig:commuteTime:example}. In this example, edge $e_{1,2}$ has
a larger commute time than all other edges in the cluster, whilst its Euclidean
distance is the same or smaller than the neighbouring Euclidean distances.

A key property of the commute time metric that allows for its use for anomaly
detection is that, by adding more data points to an existing cluster (i.e.,
making the cluster more dense), the commute time between anomalous data points
and the cluster changes dramatically whilst the average pair-wise commute times
of all nodes within the cluster is largely unaffected. This property is
illustrated in \autoref{fig:commuteTime:example2}.

\begin{figure}
    \centering
    \includegraphics[width=0.3\textwidth]{commute-time/example2}
    \caption[The commute distance from an anomaly to a node in a cluster
        increases when the cluster is denser]{The commute distance from an
        anomaly ($s$) to a node in a cluster ($t$) increases when the cluster is
        denser \cite{Khoa:2012}}
    \label{fig:commuteTime:example2}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Limitations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Limitations}
\label{commuteTime:limitations}
While commute time is a robust measure for detecting both global and local
anomalies, the main drawback is its computational time. The direct computation
of commute time involves the Eigen decomposition of the graph Laplacian matrix,
which is proportional to $O(n^3)$ and thus is not feasible for large graphs
\cite{Khoa:2012}.

It is possible, however, to sample the graph components in order to reduce the
graph size, and then an Eigenspace approximation to approximate the commute time
on the sampled graph. This is precisely how \citeauthor{Khoa:2012} developed a
anomaly detection algorithm using commute distance and Eigenspace approximation
(see \autoref{commuteTime:anomalyDetection}).

\begin{comment}
An easy way to sample a graph is selecting nodes from it uniformly at random.
However, sampling in this way can break the graph geometry structure and
anomalies may not be chosen after sampling. To resolve this, we propose a
sampling strategy called component sampling. After creating the mutual
k1-nearest neighbour graph, the graph tends to have many connected components
corresponding to different data clusters.

Anomalies are likely isolated nodes or nodes in very small components. For nodes
in normal components (we have a threshold to distinguish between normal and
anomalous components), they are uniformly sampled with the same ratio
p = 50k1/n, which is chosen from preliminary experimental results. For nodes in
anomalous components, we leave all of them intact. Then we rebuild a mutual
k1-nearest neighbour graph for the sampled data. Sampling in this way will
maintain the geometry of the original graph and the relative densities of normal
clusters. Anomalies are also not discarded in this sampling strategy.

\begin{equation}
    c_{ij} = V_G(x_i - x_j)^T (x_i - x_j)
\end{equation}
\end{comment}

The commute time between nodes on the graph can be viewed as the squared
Euclidean distance in the node space spanned by eigenvectors of the graph
Laplacian matrix.

Denote $\tilde{V}$, $\tilde{S}$ be a matrix containing $m$ largest eigenvectors
of $L^+$ and its corresponding diagonal matrix, and
$\tilde{x}_i = \tilde{S}^{-1/2}\tilde{V}^Te_i$, the approximate commute time is:
\begin{equation}
    \tilde{c}_{ij} = V_G(\tilde{x}_i - \tilde{x}_j)^T(\tilde{x}_i - \tilde{x}_j)
\end{equation}

The commute time $c_{ij}$ in an $n$ dimensional space is transformed to the
commute time $\tilde{c}_{ij}$ in an $m$ dimensional space. Therefore, we just
need to compute the $m$ smallest eigenvectors with non-zero eigenvalues of $L$
(i.e.\ the largest eigenvectors of $L^+$) to approximate the commute time.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Random Walks
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Random Walks}
\label{randomWalks}
Assume we are given a connected undirected and weighted graph $G=(V,E,W)$ with
edge weights $(w_{ij})_{i,j \in V}>=0$ be the graph adjacency matrix. A degree
of a node $i$ is $d_i=\sum_{j\in N(i)}w_{ij}$ where $N(i)$ is a set of
neighbours of node $i$. All nodes non-adjacent to $i$ are assumed to have a
weight of $w_{ij}=0$.

A random walk is a sequence of nodes on a graph visited by a random walker:
starting from a node, the random walker moves to one of its neighbours with some
probability. Then from that node, it proceeds to one of its own neighbours with
some probability, and so on \cite{Khoa:2012}. The random walk is a finite Markov
chain that is time-reversible, which means the reverse Markov chain has the same
transition probability matrix as the original Markov chain \cite{Lovasz:1996}.

The probability that a random walker selects a particular node from is
neighbours is determined by the edge weights of the graph. The larger the weight
$w_{ij}$ of the edge connecting nodes $i$ and $j$, the more often the random
walker travels through that edge.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Anomaly Detection Using Commute Time
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Anomaly Detection Using Commute Time}
\label{commuteTime:anomalyDetection}
\citeauthor{Khoa:2012} was able to devise and prove an anomaly detection
algorithm (using the commute time metric) to detect global and local outliers.
Extending this algorithm, \citeauthor{Khoa:2012} was able to use graph component
sampling and Eigenspace approximation in order to avoid the $O(n^3)$ Eigen
decomposition of the graph Laplacian. Furthermore, a pruning technique was used
in attempt to improve the run time complexity of the algorithm to $O(n \log n)$.

The algorithm proposed by \citeauthor{Khoa:2012} in \fullcite{Khoa:2012}
\cite{Khoa:2012} is presented in \autoref{algm:anomalyDetection:commuteTime}.
This section describes in detail each stage of the algorithm.

\begin{algorithm}
    \input{chapter02/anomaly-detection_commute-time.algm}
    \caption{Anomaly detection using commute time}
    \label{algm:anomalyDetection:commuteTime}
\end{algorithm}

\begin{comment}
% Construction of the k-Nearest Neighbour Graph
\subsubsection{Construction of the $k$-Nearest Neighbour Graph}
\label{anomalyDetection:commuteTime:algorithm:knnGraph}
The first stage of the algorithm is to compute the $k$-nearest neighbour graph
for the input data set.

For this purpose, the $kd$-tree technique can be applied so as to avoid an
$O(n^2)$ searching of nearest neighbours.

Algorithm
The proposed method is outlined in Algorithm 3. We create the sampled graph from the data using graph components sampling. Then the graph Laplacian $L_s$ of the sampled graph and matrix $\tilde{V}$ ($m$ smallest eigenvectors with nonzero eigenvalues of $L_s$) are computed. Since we use the pruning technique, we do not need to compute the approximate commute time for all pairs of points. Instead, we compute it `on demand'.

Algorithm 3: Commute Time Distance Based Anomaly Detection with Graph
Component Sampling and Eigenspace Approximation.
Input: Data matrix X, the numbers of nearest neighbours k1 and k2, the numbers
of anomalies to return N
Output: Top N anomalies
1: Construct the mutual k1-nearest neighbour graph from the dataset
2: Do the graph component sampling
3: Reconstruct the mutual k1-nearest neighbour graph from sampled data
4: Compute the Laplacian matrix of the sampled graph and its m smallest
eigenvectors
5: Find top N anomalies using the commute time based technique with pruning rule
(using k2)
6: Return top N anomalies

The $k$-nearest neighbour graph with $n$ nodes is built in $O(n \log n)$ using kd-tree with the assumption that the dimensionality of the data is not very high. The average degree of each node is $O(k)$ ($k << n$). So the graph is sparse and thus finding connected components take $O(kn)$. After sampling, the size of graph is $O(n_s)$ ($n_s << n$). The standard method for eigen decomposition of $L_s$ is $O({n_s}^3$. Since $L_s$ is sparse, it would take $O(Nn_s) = O(k{n_s}^2$ where $N$ is the number of nonzeros. The computation of just the $m$ smallest eigenvectors ($m < n_s$) is less expensive than that.

The typical distance-based anomaly detection takes $O({n_s}^2$) for the neighbourhood search. Pruning can scale it nearly linear [5]. We only need to compute the commute times $O(n_s)$ times each of which takes $O(m)$. So the time needed for two steps is proportional to $O(n \log n + kn + k{n_s}^2 + mn_s = O(n \log n)$, as $n_s << n$.
\end{comment}