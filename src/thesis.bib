@ARTICLE{Chandola:2007,
	institution = {University of Minnesota},
	author = {Varun Chandola AND Arindam Banerjee AND Vipin Kumar},
	title = {Anomaly Detection: A Survey},
	journaltitle = {ACM Computing Serveys},
	year = {2009},
	month = {07},
	volume = {41},
	number = {3},
	pages = {15:1--15:58},
	abstract = {Anomaly detection is an important problem that has been 
		researched within diverse research areas and application domains. Many 
		anomaly	detection techniques have been specifically developed for 
		certain application domains, while others are more generic. This survey 
		tries to provide a structured and comprehensive overview of the research
		on anomaly detection. We have grouped existing techniques into different
		categories based on the underlying approach adopted by each technique.
		For each category we have identified key assumptions, which are used by 
		the techniques to differentiate between normal and anomalous behavior.
		When applying a given technique to a particular domain, these assumptions
		can be used as guidelines to assess the effectiveness of the technique
		in that domain. For each category, we provide a basic anomaly detection
		technique, and then show how the different existing techniques in that 
		category are variants of the basic technique. This template	provides an 
		easier and succinct understanding of the techniques belonging to each 
		category. Further, for each category, we identify the advantages and 
		disadvantages of the techniques in that category. We also provide a 
		discussion on the computational complexity of the techniques since it is
		an important issue in real application domains. We hope that this survey
		will provide a better understanding of the different directions in which
		research has been done on this topic, and how techniques developed in  
		one area can be applied in domains for which they were not intended to  
		begin with.},
	publisher = {ACM},
	location = {New York, NY, USA},
	file = {:/home/joshua/University/ELEC4712/Research/001. Initial Research/Anomaly Detection.pdf:PDF},
	issn = {0360-0300},
	keywords = {Anomaly Detection, Outlier Detection, Database Applications, 
		Data Mining, Algorithms},
	url = {http://www.cs.umn.edu/tech_reports_upload/tr2007/07-017.pdf},
	urldate = {2012-05-01}
}

@INPROCEEDINGS{Chandra:1989,
	author = {Ashok K. Chandra AND Prabhakar Raghavan AND Walter L. Ruzzot AND
		Roman Smolensky AND Prasoon Tiwari},
	editor = {},
	title = {The electrical resistance of a graph captures its commute and cover
		times},
	booktitle = {Proceedings of the twenty-first annual ACM symposium on Theory of
		computing},
	year = {1989},
	series = {STOC 1989},
	pages = {574--586},
	publisher = {ACM},
	location = {New York, NY, USA},
	abstract = {View an $n$-vertex, $m$-edge undirected graph as an electrical 
		network with unit resistors as edges. We extend known relations between 
		random walks and electrical networks by showing that resistance in this 
		network is intimately connected with the lengths of random walks on the
		graph. For example, the commute time between two vertices $s$ and $t$
		(the expected length of a random walk from $s$ to $t$ and back) is
		precisely characterized by the effective resistance $R_{st}$ between $s$
		and $t$: commute time = $2mR_{st}$. Additionally, the cover time (the
		expected length of a random walk visiting all vertices) is characterized
		by the maximum resistance $R$ in the graph to within a factor of $\log
		n$: $mR ≤ cover time ≤ O (mR \log n)$. For many graphs, the bounds on
		cover time obtained in this manner are better than those obtained from 
		previous techniques such as the eigenvalues of the adjacency matrix. In
		particular, using this approach, we improve known bounds on cover times
		for various classes of graphs, including high-degree graphs, expanders, 
		and multi-dimensional meshes. Moreover, resistance seems to provide an 
		intuitively appealing and tractable approach to these problems.},
	doi = {10.1145/73007.73062},
	file = {:/home/joshua/University/ELEC4712/Research/007. Random walks on graphs/p574-chandra.pdf:PDF},
	isbn = {0-89791-307-8},
	url = {http://doi.acm.org/10.1145/73007.73062},
	urldate = {2012-05-03},
}

@BOOK{Doyle:2006,
	author = {Peter G. Doyle AND J. Laurie Snell},
	title = {Random Walks and Electric Networks},
	year = {2006},
	publisher = {Mathematical Association of America},
	location = {Washington, DC},
	file = {:/home/joshua/University/ELEC4712/Research/007. Random walks on graphs/walks.pdf:PDF},
	keywords = {networks structure, dynamics}
}

@INPROCEEDINGS{Duan:2010,
	author = {Ganglong Duan AND Zhiwen Huang AND Jianren Wang},
	editor = {},
	title = {Extreme Learning Machine for Financial Distress Prediction for 
		Listed Company},
	booktitle = {2010 International Conference on Logistics Systems and 
		Intelligent Management},
	year = {2010},
	month = {01},
	volume = {3},
	pages = {1961--1965},
	publisher = {IEEE Computer Society},
	abstract = {To overcome the shortages of the existing financial prediction 
		models such as strict hypothesis, poor generalization ability, low 
		prediction accuracy and low learning rate etc., a new early warning 
		model of financial crisis have established for listed company using 
		Extreme Learning Machine. From five dimensions of solvency, 
		operating-ability, profitability, cash-ability and grow-ability, fifteen
		financial indexes were selected as the input variables; and the output 
		variable was defined as whether the listed company had been special 
		treated or not. The empirical analysis results show the training and 
		validation accuracy of the model are 100\% and 92\% respectively, which 
		concludes that learning and generalization abilities of this model are 
		excellent, and which can meet the requirements of financial distress 
		prediction for listed company.},
	doi = {10.1109/ICLSIM.2010.5461268},
	file = {:/home/joshua/University/ELEC4712/Research/001. Initial Research/Extreme Learning Machine for Financial Distress Prediction for Listed Company.pdf:PDF},
	isbn = {978-1-4244-7331-1},
	keywords = {Extreme Learning Machine, Data Mining, Financial Crisis, 
		Early-warning Mode},
	urldate = {2012-05-01}
}

@ARTICLE{Fouss:2007,
	author = {Francois Fouss AND Alain Pirotte AND Jean-Michel Renders AND Marco
		Saerens},
	title = {Random-Walk Computation of Similarities between Nodes of a Graph
		with Application to Collaborative Recommendation},
	journaltitle = {IEEE Trans. on Knowl. and Data Eng.},
	year = {2007},
	month = {03},
	volume = {19},
	number = {3},
	pages = {355--369},
	abstract = {This work presents a new perspective on characterizing the 
		similarity between elements of a database or, more generally, nodes of a
		weighted and undirected graph. It is based on a Markov-chain model of 
		random walk through the database. More precisely, we compute quantities
		(the average commute time, the pseudoinverse of the Laplacian matrix of
		the graph, etc) that provide similarities between any pair of nodes, 
		having the nice property of increasing when the number of paths 
		connecting those elements increases and when the ``length'' of paths 
		decreases. It turns out that the square root of the average commute time
		is a Euclidean distance and that the pseudoinverse of the Laplacian 
		matrix is a kernel (its elements are inner products closely related to 
		commute times). A procedure for computing the subspace projection of the
		node vectors of the graph that preserves as much variance as possible in
		terms of the commute-time distance --- a principal component analysis 
		(PCA) of the graph --- is also introduced. This graph PCA provides a 
		nice interpretation to the ``Fiedler vector'', widely used for graph 
		partitioning. The model is evaluated on a collaborative-recommendation 
		task where suggestions are made about which movies people should watch 
		based upon what they watched in the past. Experimental results on the 
		MovieLens database show that the Laplacian-based similarities perform 
		well in comparison with other methods. The model, which nicely fits into
		the so-called ``statistical relational learning'' framework, could also 
		be used to compute document or word similarities, and, more generally, 
		it could be applied to machine-learning and pattern-recognition tasks 
		involving a database.},
	location = {Piscataway, NJ, USA},
	doi = {10.1109/TKDE.2007.46},
	file = {:/home/joshua/University/ELEC4712/Research/007. Random walks on graphs/10.1.1.71.2194.pdf:PDF},
	issn = {1041-4347},
	keywords = {Graph analysis, graph and database mining, collaborative 
		recommendation,	graph kernels, spectral clustering, Fiedler vector, 
		proximity measures,	statistical relational learning.},
	publisher = {IEEE Educational Activities Department}
}

@ARTICLE{Goldberger:2000,
	author = {Ary L. Goldberger AND Luis A. N. Amaral AND Leon Glass AND Jeffrey
		M. Hausdorff AND Plamen Ch. Ivanov AND Roger G. Mark AND Joseph E.
		Mietus AND George B. Moody AND Chung-Kang Peng AND H. Eugene Stanley},
	title = {{PhysioBank, PhysioToolkit, and PhysioNet}: Components of a new 
		research resource for complex physiologic signals},
	journaltitle = {Circulation},
	year = {2000},
	volume = {101},
	number = {23},
	pages = {e215--e220},
	abstract = {The newly inaugurated Research Resource for Complex Physiologic 
		Signals, which was created under the auspices of the National Center for
		Research Resources of the National Institutes of Health, is intended to 
		stimulate current research and new investigations in the study of 
		cardiovascular and other complex biomedical signals. The resource has 3 
		interdependent components. PhysioBank is a large and growing archive of 
		well-characterized digital recordings of physiological signals and 
		related data for use by the biomedical research community. It currently 
		includes databases of multiparameter cardiopulmonary, neural, and other 
		biomedical signals from healthy subjects and from patients with a 
		variety of conditions with major public health implications, including 
		life-threatening arrhythmias, congestive heart failure, sleep apnea, 
		neurological disorders, and aging. PhysioToolkit is a library of 
		open-source software for physiological signal processing and analysis, 
		the detection of physiologically significant events using both classic 
		techniques and novel methods based on statistical physics and nonlinear 
		dynamics, the interactive display and characterization of signals, the 
		creation of new databases, the simulation of physiological and other 
		signals, the quantitative evaluation and comparison of analysis methods,
		and the analysis of nonstationary processes. PhysioNet is an on-line
		forum for the dissemination and exchange of recorded biomedical signals
		and open-source software for analyzing them. It provides facilities for
		the cooperative analysis of data and the evaluation of proposed new
		algorithms. In addition to providing free electronic access to
		PhysioBank data and PhysioToolkit software via the World Wide Web
		(\url{http://www.physionet.org}), PhysioNet offers services and training
		via on-line tutorials to assist users with varying levels of expertise.},
	doi = {10.1161/01.CIR.101.23.e215},
	file = {:/home/joshua/University/ELEC4712/Research/005. Matrices/Circulation-2000-Goldberger-e215-20.pdf:PDF},
	keywords = {aging, databases, death, sudden, electrophysiology, heart rate, 
		nervous system, autonomic, nonlinear dynamics},
	url = {http://circ.ahajournals.org/content/101/23/e215.full},
	urldate = {2012-05-01},
}

@BOOK{Golub1996,
	author = {Gene Howard Golub AND Charles F. Van Loan},
	title = {Matrix Computations},
	publisher = {Johns Hopkins University Press},
	year = {1996},
	edition = {3},
	abstract = {Revised and updated, the third edition of Golub and Van Loan's 
		classic text in computer science provides essential information about 
		the mathematical background and algorithmic skills required for the 
		production of numerical software. This new edition includes thoroughly 
		revised chapters on matrix multiplication problems and parallel matrix 
		computations, expanded treatment of CS decomposition, an updated 
		overview of floating point arithmetic, a more accurate rendition of the 
		modified Gram-Schmidt process, and new material devoted to GMRES, QMR, 
		and other methods designed to handle the sparse unsymmetric linear 
		system problem.},
	isbn = {9780801854149}
}

@BOOK{Grinstead:1997,
	author = {Charles M. Grinstead AND J. Laurie Snell},
	title = {Introduction to Probability},
	publisher = {American Mathematical Society},
	year = {1997},
	edition = {2},
	pages = {405--470},
	abstract = {Text is designed for an introductory probability course at the 
		university level for sophomores, juniors, and seniors in mathematics, 
		physical and social sciences, engineering, and computer science. It 
		presents a thorough treatment of ideas and techniques necessary for a 
		firm understanding of the subject. The text is also recommended for use
		in discrete probability courses. The material is organized so that the
		discrete and continuous probability discussions are presented in a 
		separate, but parallel, manner. This organization does not emphasize an 
		overly rigorous or formal view of probabililty and therefore offers some
		strong pedagogical value. Hence, the discrete discussions can sometimes 
		serve to motivate the more abstract continuous probability discussions. 
		\textbf{Features:} Key ideas are developed in a somewhat leisurely 
		style, providing a variety of interesting applications to probability 
		and showing some nonintuitive ideas. Over 600 exercises provide the 
		opportunity for practicing skills and developing a sound understanding
		of ideas. Numerous historical comments deal with the development of 
		discrete probability. The text includes many computer programs that 
		illustrate the algorithms or the methods of computation for important 
		problems.},
	file = {:/home/joshua/University/ELEC4712/Research/006. Markov Chains/Chapter11.pdf:PDF},
	isbn = {978-0821807491},
	url = {http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/amsbook.mac.pdf},
	urldate = {2012-05-03}
}

@BOOK{Hauck:2007,
	author = {Scott Hauck AND Andr\'{e} DeHon},
	title = {Reconfigurable Computing: The Theory and Practice of FPGA-Based 
		Computation (Systems on Silicon)},
	publisher = {Morgan Kaufmann Publishers},
	year = {2007},
	edition = {1},
	abstract = {The main characteristic of Reconfigurable Computing is the 
		presence of hardware that can be reconfigured to implement specific 
		functionality more suitable for specially tailored hardware than on a 
		simple uniprocessor. Reconfigurable computing systems join 
		microprocessors and programmable hardware in order to take advantage of 
		the combined strengths of hardware and software and have been used in 
		applications ranging from embedded systems to high performance 
		computing. Many of the fundamental theories have been identified and 
		used by the Hardware/Software Co-Design research field. Although the 
		same background ideas are shared in both areas, they have different 
		goals and use different approaches. This book is intended as an 
		introduction to the entire range of issues important to reconfigurable 
		computing, using FPGAs as the context, or ``computing vehicles'' to 
		implement this powerful technology. It will take a reader with a 
		background in the basics of digital design and software programming and 
		provide them with the knowledge needed to be an effective designer or 
		researcher in this rapidly evolving field.},
	isbn = {978-0123705228}
}

@BOOK{Jolliffe:2002,
	author = {I.T. Jolliffe},
	title = {Principal Component Analysis},
	publisher = {Springer},
	year = {2002},
	edition = {2},
	file = {:/home/joshua/University/ELEC4712/Research/009. Principal Component Analysis/Jolliffe I. Principal Component Analysis (2ed., Springer, 2002)(518s)_MVsa_.pdf:PDF},
	isbn = {978-0387954424}
}

@PHDTHESIS{Khoa:2012,
	author = {Nguyen Lu Dang Khoa},
	title = {Large Scale Anomaly Detection and Clustering Using Random Walks},
	institution = {University of Sydney},
	year = {2012},
	month = {03},
	abstract = {Many data mining and machine learning tasks involve calculating 
		`distances' between data objects. Euclidean distance is the most widely 
		used metric. However, there are situations where the Euclidean distance
		or other traditional metrics such as Mahalanobis distance and graph
		geodesic distance are not suitable to use as a measure of distance.

		Commute time is a robust measure derived from a random walk on graphs.
		In this thesis, we present methods to use commute time as a distance
		measure for data mining tasks such as anomaly detection and clustering.
		However, the computation of commute time involves the eigen 
		decomposition of the graph Laplacian and thus is impractical for large 
		graphs. We also propose methods to efficiently and accurately compute 
		commute time in batch and incremental fashions.},
	file = {:/home/joshua/University/ELEC4712/Research/003. Khoa/Large Scale Anomaly Detection and Clustering Using Random Walks.pdf:PDF}
}

@INPROCEEDINGS{Khoa:2010,
	author = {Nguyen Lu Dang Khoa AND Sanjay Chawla},
	editor = {},
	title = {Robust Outlier Detection Using Commute Time and Eigenspace 
		Embedding},
	booktitle = {PAKDD}, 
	year = {2010},
	abstract = {We present a method to ﬁnd outliers using `commute distance' 
		computed from a random walk on graph. Unlike Euclidean distance, commute
		distance between two nodes captures both the distance between them and 
		their local neighborhood densities. Indeed commute distance is the 
		Euclidean distance in the space spanned by eigenvectors of the graph 
		Laplacian matrix. We show by analysis and experiments that using this 
		measure, we can capture both global and local outliers eﬀectively with 
		just a distance based method. Moreover, the method can detect outlying
		clusters which other traditional methods often fail to capture and also
		shows a high resistance to noise than local outlier detection method. 
		Moreover, to avoid the $O(n^{3})$ direct computation of commute	
		distance, a graph component sampling and an eigenspace approximation
		combined with pruning technique reduce the time to $O(n \log n)$ while
		preserving the outlier ranking.},
	doi = {10.1007/978-3-642-13672-6_41},
	file = {:/home/joshua/University/ELEC4712/Research/002. Chawla/Khoa_pakdd10.pdf:PDF},
	keywords = {outlier detection, commute distance, eigenspace embedding, 
		random walk, nearest neighbor graph},
	pages = {422--434}, 
	volume = {2}
}

@ARTICLE{Knorr:2000,
	author = {Edwin M. Knorr AND Raymond T. Ng AND Vladimir Tucakov},
	title = {Distance-based outliers: algorithms and applications},
	journaltitle = {The VLDB Journal --- The International Journal on Very Large
		Data Bases},
	year = {2000},
	volume = {8},
	pages = {237--253},
	number = {3--4},
	month = {02},
	abstract = {This paper deals with finding outliers (exceptions) in large, 
		multidimensional datasets. The identification of outliers can lead to 
		the discovery of truly unexpected knowledge in areas such as electronic 
		commerce, credit card fraud, and even the analysis of performance 
		statistics of professional athletes. Existing methods that we have seen 
		for finding outliers can only deal efficiently with two 
		dimensions/attributes of a dataset. In this paper, we study the notion 
		of DB (distance-based) outliers. Specifically, we show that (i) outlier 
		detection can be done efficiently for large datasets, and for 
		k-dimensional datasets with large values of $k$ (e.g., $k \ge 5$); and 
		(ii), outlier detection is a meaningful and important knowledge 
		discovery task. First, we present two simple algorithms, both having a 
		complexity of $O(k \: N^2)$, $k$ being the dimensionality and $N$ being 
		the number of objects in the dataset. These algorithms readily support 
		datasets with many more than two attributes. Second, we present an 
		optimized cell-based algorithm that has a complexity that is linear with
		respect to $N$, but exponential with respect to $k$. We provide 
		experimental results indicating that this algorithm significantly 
		outperforms the two	simple algorithms for $k \leq 4$. Third, for 
		datasets that are mainly disk-resident, we present another version of 
		the cell-based algorithm that guarantees at most three passes over a 
		dataset. Again, experimental results show that this algorithm is by far 
		the best for $k \leq 4$. Finally, we discuss our work on three real-life
		applications, including one on spatio-temporal data (e.g., a video 
		surveillance application), in order to confirm the relevance and broad 
		applicability of DB outliers.},
	doi = {10.1007/s007780050006}
}

@INPROCEEDINGS{Lei:2010,
	author = {Shi Lei AND Ma Xinming AND Xi Lei AND Hu Xiaohong},
	editor = {},
	title = {Financial Data Mining based on Support Vector Machines and Ensemble
		Learning},
	booktitle = {Proceedings of the 2010 International Conference on Intelligent 
		Computation	Technology and Automation},
	volume = {2},
	series = {ICICTA 2010},
	pages = {313--314},
	year = {2010},
	month = {05},
	publisher = {IEEE Computer Society},
	location = {Washington, DC, USA},
	abstract = {With the rapid development of e-commerce, financial data mining 
		has been one of the most important research topics in the data mining
		community. Support vector machines (SVMs) and ensemble learning are two
		popular techniques in the machine learning field. In this paper, support
		vector machines and ensemble learning are used to classify financial 
		data respectively. The experiments conducted on the public dataset show
		that compared with SVMs, ensemble learning achieves obvious improvement 
		of performance.},
	doi = {10.1109/ICICTA.2010.787},
	file = {:/home/joshua/University/ELEC4712/Research/001. Initial Research/Financial Data Mining based on Support Vector Machines and Ensemble Learning.pdf:PDF},
	isbn = {978-0-7695-4077-1},
	keywords = {Financial Data Mining, Support Vector Machines, Ensemble 
		Learning}
}

@INPROCEEDINGS{Li:2007,
	author = {Xiuquan Li AND Zhidong Deng},
	editor = {},
	title = {A Machine Learning Approach to Predict Turning Points for Chaotic
		Financial Time Series},
	booktitle = {19th IEEE International Conference on Tools with Artificial 
		Intelligence},
	year = {2007},
	month = {10},
	volume = {2},
	pages = {331--335},
	publisher = {IEEE Computer Society},
	abstract = {In this paper, a novel approach to predict turning points for 
		chaotic financial time series is proposed based on chaotic theory and 
		machine learning. The nonlinear mapping between different data points in
		primitive time series is derived and proven. Our definition of turning
		points produces an event characterization function, which can transform
		the profile of time series to a measure. The RBF neural network is
		further used as a nonlinear modeler. We discuss the threshold selection
		and give a procedure for threshold estimation using out-of-sample
		validation. The proposed approach is applied to the prediction problem
		of two real-world financial time series. The experimental results
		validate the effectiveness of our new approach.},
	doi = {10.1109/ICTAI.2007.105},
	file = {:/home/joshua/University/ELEC4712/Research/001. Initial Research/A Machine Learning Approach to Predict Turning Points for Chaotic Financial Time Series.pdf:PDF}
}

@INCOLLECTION{Lovasz:1996,
	author = {L. Lov\'asz},
	editor = {D. {Mikl\'os} AND V. T. {S\'os} AND T. {Sz\H{o}nyi}},
	title = {Random Walks on Graphs: A Survey},
	booktitle = {Combinatorics, Paul Erd\H{o}s is Eighty},
	year = {1996},
	volume = {2},
	pages = {353--398},
	publisher = {J\'anos Bolyai Mathematical Society},
	location = {Budapest},
	abstract = {Various aspects of the theory of random walks on graphs are 
		surveyed. In particular, estimates on the important parameters of access
		time, commute time,	cover time and mixing time are discussed Connections
		with the eigenvalues of graphs and with electrical networks and the use 
		of these connections in the study of random walks is described We also 
		sketch recent algorithmic applications of random walks, in particular to
		the problem of sampling.},
	file = {:/home/joshua/University/ELEC4712/Research/007. Random walks on graphs/RandomWalks.pdf:PDF},
	keywords = {networks structure dynamics}
}

@BOOK{Maimon:2005,
	author = {Oded Z Maimon AND Lior Rokach},
	title = {Data Mining and Knowledge Discovery Handbook: A Complete Guide for
		Practitioners and Researchers},
	year = {2005},
	pages = {1--16},
	publisher = {Kluwer Academic Publishers},
	abstract = {Data Mining and Knowledge Discovery Handbook organizes all major
		concepts, theories, methodologies, trends, challenges and applications 
		of data mining (DM) and knowledge discovery in databases (KDD) into a 
		coherent and unified repository. This book first surveys, then provides 
		comprehensive yet concise algorithmic descriptions of methods, including
		classic methods plus the extensions and novel methods developed 
		recently. This volume concludes with in-depth descriptions of data 
		mining applications in various interdisciplinary industries including 
		finance, marketing, medicine, biology, engineering, telecommunications, 
		software, and security. Data Mining and Knowledge Discovery Handbook is 
		designed for research scientists and graduate-level students in computer
		science and engineering.},
	file = {:/home/joshua/University/ELEC4712/Research/008. Outlier detection/outlier.pdf:PDF},
	isbn = {9780387244358}
}

@ARTICLE{Moonesinghe:2008,
	author = {H. D. K. Moonesinghe AND Pang-Ning Tan},
	title = {Outrank: a Graph-Based Outlier Detection Framework Using Random 
		Walk},
	journaltitle = {International Journal on Artificial Intelligence Tools},
	year = {2008},
	volume = {17},
	number = {1},
	pages = {19--36},
	abstract = {This paper introduces a stochastic graph-based algorithm, called
		OutRank, for detecting outliers in data. We consider two approaches for 
		constructing a graph representation of the data, based on the object 
		similarity and number of shared neighbors between objects. The heart of 
		this approach is the Markov chain model that is built upon this graph,
		which assigns an outlier score to each object. Using this framework, we
		show that our algorithm is more robust than the existing outlier 
		detection schemes and can effectively address the inherent problems of
		such schemes. Empirical studies conducted on both real and synthetic 
		data sets show that significant improvements in detection rate and false
		alarm rate are achieved using the proposed framework.},
	doi = {10.1142/S0218213008003753},
	file = {:/home/joshua/University/ELEC4712/Research/007. Random walks on graphs/IJAIT.pdf:PDF},
	keywords = {Outlier detection, random walk, Markov chain}
}

@INPROCEEDINGS{Nakayama:2000,
	author = {Hirotaka Nakayama AND Kengo Yoshii},
	editor = {},
	title = {Active Forgetting in Machine Learning and its Application to 
		Financial Problems},
	booktitle = {IJCNN},
	year = {2000},
	month = {07},
	volume = {5},
	pages = {123--128},
	organization = {IEEE},
	abstract = {One of main features in financial investment problems is that 
		the situation changes very often over time. Under this circumstance,
		in particular, it has been observed that additional learning plays an
		effective role. However, since the rule for classification becomes more
		and more complex with only additional learning, some appropriate 
		forgetting is also necessary. It seems natural that many data are
		forgotten as the time elapses. On the other hand, it is expected more
		effective to forget unnecessary data actively. In this paper, several
		methods for active forgetting are suggested. The effectiveness of active
		forgetting is shown by examples in stock portfolio problems.},
	doi = {10.1109/IJCNN.2000.861445},
	file = {:/home/joshua/University/ELEC4712/Research/001. Initial Research/Active Forgetting in Machine Learning and its Application to Financial Problems.pdf:PDF},
	keywords = {pattern classification, potential method, additional learning, 
		forgetting}
}

@MISC{Pati:2011,
	author = {Sukanta Pati},
	title = {Laplacian matrix of a graph},
	year = {2011},
	howpublished = {\url{http://www.lix.polytechnique.fr/~schwander/resources/mig/slides/pati.pdf}},
	note = {Talk prepared for the Indo-French workshop, 2011},
	file = {:/home/joshua/University/ELEC4712/Research/005. Matrices/pati.pdf:PDF},
	urldate = {2012-05-01},
}

@INPROCEEDINGS{Coyne:2011,
	author = {Robin Pottathuparambil AND Jack Coyne AND Jeffrey Allred AND 
		William Lynch AND Vincent Natoli},
	title = {Low-latency FPGA Based Financial Data Feed Handler},
	maintitle = {IEEE International Symposium on Field-Programmable Custom 
		Computing Machines},
	booktitle = {Proceedings of the 2011 IEEE 19th Annual International 
		Symposium on Field-Programmable Custom Computing Machines},
	year = {2011},
	month = {05},
	series = {FCCM 2011},
	pages = {93--96},
	publisher = {IEEE Computer Society},
	location = {Washington, DC, USA},
	abstract = {Financial exchanges provide real time data feeds containing 
		trade, order and status information to brokers, traders and other market
		makers. ITCH is one such market data feed that is disseminated by the
		NASDAQ exchange. The work presented in this paper describes an FPGA 
		based ITCH feed handler and processing system. The handler, built on the
		Stone Ridge RDX-11 hardware platform with a combination of HDL and 
		Impulse C, accepts and processes ITCH packet sat line speed with 
		extremely low latency. Our implementation parses sixteen different stock
		symbols in the feed and generates an outbound packet on the NASDAQ one 
		second heartbeat. The unit was tested with an artificial feed that could
		be adjusted in speed to simulate market surges. The system demonstrated 
		a turnaround latency of $2.7\mu{}s$ with very little variation for all 
		tested feed rates. The CPU equivalent demonstrated $38 \pm 22 \mu{}s$ 
		(1x rate) with a long tail illustrating the variability inherent to
		processing by the host O/S. The FPGA solution demonstrated ultra-low,
		deterministic latency and was able to continue processing data at the 
		line rate limit.},
	doi = {10.1109/FCCM.2011.50},
	file = {:/home/joshua/University/ELEC4712/Research/001. Initial Research/Low-latency FPGA Based Financial Data Feed Handler.pdf:PDF},
	isbn = {978-0-7695-4301-7},
	keywords = {FPGA, Low-latency Trading, ITCH feed handler}
}

@MISC{Spielman:2009a,
	author = {Daniel A. Spielman},
	title = {The Laplacian},
	year = {2009},
	month = {09},
	howpublished = {\url{http://www.cs.yale.edu/homes/spielman/561/lect02-09.pdf}},
	note = {Lecture notes},
	file = {:/home/joshua/University/ELEC4712/Research/005. Matrices/lect02-09.pdf:PDF},
	urldate = {2012-05-01}
}

@ARTICLE{Spielman:2006,
	author = {Daniel A. Spielman AND Shang-Hua Teng},
	title = {Nearly-Linear Time Algorithms for Preconditioning and Solving 
		Symmetric, Diagonally Dominant Linear Systems},
	journaltitle = {CoRR},
	volume = {abs/cs/0607105},
	year = {2006},
	month = {09},
	file = {:/home/joshua/University/ELEC4712/Research/004. Speilman-Teng/Nearly-Linear Time Algorithms for Preconditioning and Solving Symmetric, Diagonally Dominant Linear Systems.pdf:PDF},
	keywords = {Numerical Analysis, Data Structures and Algorithms},
	url = {http://arxiv.org/pdf/cs/0607105v4.pdf},
	urldate = {2012-05-01},
}

@INPROCEEDINGS{Chawla:2004,
	author = {Pei Sun AND Sanjay Chawla},
	title = {On Local Spatial Outliers},
	booktitle = {ICDM},
	year = {2004},
	pages = {209--216},
	abstract = {We propose a measure, Spatial Local Outlier Measure (SLOM) which
		captures the local behaviour of datum in their spatial neighborhood. 
		With the help of SLOM we are able to discern local spatial outliers 
		which are usually missed by global techniques like ``three standard 
		deviations away from the mean''. Furthermore the measure takes into 
		account the local stability around a data point and supresses the 
		reporting of outliers in highly unstable areas, where data is too 
		heterogeneous and the notion of outliers is not meaningful. We prove 
		several properties of SLOM and report experiments on synthetic and real 
		data sets which show that our approach is novel and scalable to large 
		data sets.},
	doi = {10.1109/ICDM.2004.10097},
	file = {:/home/joshua/University/ELEC4712/Research/002. Chawla/icdm04.pdf:PDF}
}

@MISC{Berkeley:1999,
	author = {University of California, Berkeley},
	title = {Graph Partitioning, Part 2},
	year = {1999},
	month = {04},
	howpublished = {\url{http://www.cs.berkeley.edu/~demmel/cs267/lecture20/lecture20.html}},
	note = {CS267 Lecture Notes},
	urldate = {2012-05-01},
}

@INPROCEEDINGS{Vries:2010,
	author = {Timothy de Vries AND Sanjay Chawla AND Michael Houle},
	title = {Finding Local Anomalies in Very High Dimensional Space},
	booktitle = {10th IEEE International Conference on Data Mining},
	year = {2010},
	pages = {128--137},
	doi = {10.1109/ICDM.2010.151},
	file = {:/home/joshua/University/ELEC4712/Research/002. Chawla/devries_icdm10.pdf:PDF}
}

@ARTICLE{Vries:2011,
	author = {Timothy de Vries AND Sanjay Chawla AND Michael E. Houle},
	title = {Density-preserving projections for large-scale local anomaly 
		detection},
	journaltitle = {Knowledge and Information Systems},
	year = {2011},
	month = {06},
	abstract = {Outlier or anomaly detection is a fundamental data mining task 
		with the aim to identify data points, events, transactions which deviate
		from the norm. The identification of outliers in data can provide 
		insights about the underlying data generating process. In general, 
		outliers can be of two kinds: global and local. Global outliers are
		distinct with respect to the whole data set, while local outliers are
		distinct with respect to data points in their local neighbourhood. While
		several approaches have been proposed to scale up the process of global
		outlier discovery in large databases, this has not been the case for 
		local outliers. We tackle this problem by optimising the use of local 
		outlier factor (LOF) for large and high-dimensional	data. We propose 
		projection-indexed nearest-neighbours (PINN), a novel technique that 
		exploits extended nearest-neighbour sets in a reduced-dimensional space 
		to create an accurate approximation for k-nearest-neighbour distances, 
		which is used as the core density measurement within LOF. The reduced 
		dimensionality allows for efficient sub-quadratic indexing in the number
		of items in the data set, where previously only quadratic performance 
		was possible. A detailed theoretical analysis of random projection (RP) 
		and PINN shows that we are able to preserve the density of the intrinsic
		manifold of the data set after projection. Experimental results show 
		that PINN outperforms the standard projection methods RP and PCA when 
		measuring LOF for many high-dimensional real-world data sets of up to 
		300,000 elements and 102,600 dimensions. A further investigation into 
		the use of high-dimensionality-specific indexing such as spatial 
		approximate sample hierarchy (SASH) shows that our novel technique holds
		benefits over even these types of highly efficient indexing. We cement 
		the practical applications of our novel technique with insights into 
		what it means to find local outliers in real data including image and 
		text data, and include potential applications for this knowledge.},
	doi = {10.1007/s10115-011-0430-4},
	file = {:/home/joshua/University/ELEC4712/Research/002. Chawla/Density-preserving projections for large-scale local anomaly detection.pdf:PDF},
	keywords = {Anomaly detection, Dimensionality reduction}
}

@INPROCEEDINGS{Vuillemin:2002,
	author = {Jean E. Vuillemin AND Patrice Bertin AND Didier Roncin AND Mark 
		Shand AND Herv{\'e} H. Touati AND Philippe Boucard},
	editor = {Giovanni De Micheli AND Rolf Ernst AND Wayne Wolf},
	maintitle = {IEEE Transactions on VLSI Systems},
	booktitle = {Readings in hardware/software co-design},
	year = {2002},
	pages = {611--624},
	publisher = {Kluwer Academic Publishers},
	location = {Norwell, MA, USA},
	chapter = {Programmable active memories: reconfigurable systems come of age},
	isbn = {1-55860-702-1},
	keywords = {FPGA, PAM, field-programmable gate array, programmable active 
		memory,	reconfigurable system}
}

@INPROCEEDINGS{Yoo:2005,
	author = {Paul D. Yoo AND Maria H. Kim AND Tony Jan},
	title = {Financial Forecasting: Advanced Machine Learning Techniques in 
		Stock Market Analysis},
	booktitle = {9th International Multitopic Conference},
	year = {2005},
	month = {12},
	pages = {1--7},
	organization = {IEEE},
	abstract = {The prediction of stock market has been an important issue in 
		the field of finance, mathematics and engineering due to its great 
		potential financial gain. In addition, uncertainty in the prediction of 
		the	financial time series has attracted interest from many researchers.
		In this study, we present recent developments in stock market prediction
		models, and discuss their strengths and limitations. In addition, we 
		investigate diverse macroeconomic factors and their issues in the 
		prediction of stock market. From this study, we found that incorporating
		event information into the prediction models plays important roles for 
		more accurate prediction. Hence, an accurate event weighting method and 
		a stable automated event extraction system are required for more 
		accurate and reliable stock market prediction.},
	doi = {10.1109/INMIC.2005.334420},
	isbn = {0-7803-9429-1},
	file = {:/home/joshua/University/ELEC4712/Research/001. Initial Research/Financial Forecasting - Advanced Machine Learning Techniques in Stock Market Analysis.pdf:PDF}
}

@ARTICLE{Yu:2009,
	author = {Lean Yu AND Huanhuan Chen AND Shouyang Wang AND Kin Keung Lai},
	title = {Evolving Least Squares Support Vector Machines for Stock Market 
		Trend Mining},
	journaltitle = {Trans. Evol. Comp},
	maintitle = {IEEE Transactions of Evolutionary Computation},
	year = {2009},
	month = {02},
	volume = {13},
	number = {1},
	pages = {87--102},
	abstract = {In this paper, an evolving least squares support vector machine 
		(LSSVM) learning paradigm with a mixed kernel is proposed to explore 
		stock market trends. In the proposed learning paradigm, a genetic 
		algorithm (GA), one of the most popular evolutionary algorithms (EAs), 
		is first used to select input features for LSSVM learning, i.e., 
		evolution of input features. Then, another GA is used for parameters
		optimization of LSSVM, i.e., evolution of algorithmic parameters. 
		Finally, the evolving LSSVM learning paradigm with best feature subset, 
		optimal parameters, and a mixed kernel is used to predict stock market 
		movement direction in terms of historical data series. For illustration 
		and evaluation purposes, three important stock indices, S&P 500 Index,
		Dow Jones Industrial Average (DJIA) Index, and New York Stock Exchange
		(NYSE) Index, are used as testing targets. Experimental results obtained
		reveal that the proposed evolving LSSVM can produce some forecasting
		models that are easier to be interpreted by using a small number of
		predictive features and are more efficient than other parameter 
		optimization methods. Furthermore, the produced forecasting model can
		significantly outperform other forecasting models listed in this paper
		in terms of the hit ratio. These findings imply that the proposed 
		evolving LSSVM learning paradigm can be used as a promising approach to
		stock market tendency exploration.},
	publisher = {IEEE Press},
	location = {Piscataway, NJ, USA},
	doi = {10.1109/TEVC.2008.928176},
	file = {:/home/joshua/University/ELEC4712/Research/001. Initial Research/Evolving Least Squares Support Vector Machines for Stock Market Trend Mining.pdf:PDF},
	issn = {1089-778X},
	keywords = {artificial neural networks (ANNs), evolutionary algorithms 
		(EAs), feature selection, genetic algorithm (GA), least squares support
		vector machine (LSSVM), mixed kernel, parameter optimization, 
		statistical models, stock market trend mining}
}

@BOOK{Hawkins:1980,
	author = {Douglas M. Hawkins},
	title = {Identification of outliers},
	series = {Monographs on Applied Probability and Statistics},
	year = {1980},
	publisher = {Chapman and Hall},
	isbn = {9780412219009}
}
