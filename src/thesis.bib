%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% dir = {001. Initial Research}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% file = {Active Forgetting in Machine Learning and its Application to Financial Problems.pdf}
@PROCEEDINGS{Nakayama:2000,
    author = {Hirotaka Nakayama AND Kengo Yoshii},
    title = {Active Forgetting in Machine Learning and its Application to
        Financial Problems},
    journal = {Proceedings of the IEEE-INNS-ENNS International Joint Conference
        on Neural Networks, 2000. IJCNN 2000},
    year = {2000},
    volume = {5},
    pages = {123--128},
    abstract = {One of main features in financial investment problems is that
        the situation changes very often over time. Under this circumstance, in
        particular, it has been observed that additional learning plays an
        effective role. However, since the rule for classification becomes more
        and more complex with only additional learning, some appropriate
        forgetting is also necessary. It seems natural that many data are
        forgotten as the time elapses. On the other hand, it is expected more
        effective to forget unnecessary data actively. In this paper, several
        methods for active forgetting are suggested. The effectiveness of active
        forgetting is shown by examples in stock portfolio problems},
    keywords = {forgetting, machine learning},
    url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=861445},
    urldate = {2012-08-22}
}

% file = {A Machine Learning Approach to Predict Turning Points for Chaotic Financial Time Series.pdf}
@INPROCEEDINGS{Li:2007,
    author = {Xiuquan Li AND Zhidong Deng},
    title = {A Machine Learning Approach to Predict Turning Points for Chaotic
        Financial Time Series},
    booktitle = {Proceedings of the 19th IEEE International Conference on Tools
        with Artificial Intelligence},
    series = {ICTAI 2007},
    year = {2007},
    volume = {2},
    pages = {331--335},
    numpages = {5},
    abstract = {In this paper, a novel approach to predict turning points for
        chaotic financial time series is proposed based on chaotic theory and
        machine learning. The nonlinear mapping between different data points in
        primitive time series is derived and proven. Our definition of turning
        points produces an event characterization function, which can transform
        the profile of time series to a measure. The RBF neural network is
        further used as a nonlinear modeler. We discuss the threshold selection
        and give a procedure for threshold estimation using out-of-sample
        validation. The proposed approach is applied to the prediction problem
        of two real-world financial time series. The experimental results
        validate the effectiveness of our new approach.},
    publisher = {IEEE Computer Society},
    location = {Washington, DC, USA},
    doi = {10.1109/ICTAI.2007.18},
    isbn = {0-7695-3015-X},
    url = {http://dx.doi.org/10.1109/ICTAI.2007.18},
    urldate = {2012-08-22}
}

% file = {Evolving Least Squares Support Vector Machines for Stock Market Trend Mining.pdf}
@ARTICLE{Yu:2009,
    author = {Lean Yu AND Huanhuan Chen AND Shouyang Wang AND Kin Keung Lai},
    title = {Evolving Least Squares Support Vector Machines for Stock Market
        Trend Mining},
    journaltitle = {Trans. Evol. Comp},
    maintitle = {IEEE Transactions of Evolutionary Computation},
    year = {2009},
    month = {02},
    volume = {13},
    number = {1},
    pages = {87--102},
    abstract = {In this paper, an evolving least squares support vector machine
        (LSSVM) learning paradigm with a mixed kernel is proposed to explore
        stock market trends. In the proposed learning paradigm, a genetic
        algorithm (GA), one of the most popular evolutionary algorithms (EAs),
        is first used to select input features for LSSVM learning, i.e.,
        evolution of input features. Then, another GA is used for parameters
        optimization of LSSVM, i.e., evolution of algorithmic parameters.
        Finally, the evolving LSSVM learning paradigm with best feature subset,
        optimal parameters, and a mixed kernel is used to predict stock market
        movement direction in terms of historical data series. For illustration
        and evaluation purposes, three important stock indices, S&P 500 Index,
        Dow Jones Industrial Average (DJIA) Index, and New York Stock Exchange
        (NYSE) Index, are used as testing targets. Experimental results obtained
        reveal that the proposed evolving LSSVM can produce some forecasting
        models that are easier to be interpreted by using a small number of
        predictive features and are more efficient than other parameter
        optimization methods. Furthermore, the produced forecasting model can
        significantly outperform other forecasting models listed in this paper
        in terms of the hit ratio. These findings imply that the proposed
        evolving LSSVM learning paradigm can be used as a promising approach to
        stock market tendency exploration.},
    publisher = {IEEE Press},
    location = {Piscataway, NJ, USA},
    doi = {10.1109/TEVC.2008.928176},
    issn = {1089-778X},
    keywords = {artificial neural networks, evolutionary algorithms, feature
        selection, genetic algorithm, least squares support vector machine,
        mixed kernel, parameter optimization, statistical models, stock market
        trend mining}
}

% file = {Extreme Learning Machine for Financial Distress Prediction for Listed Company.pdf}
@INPROCEEDINGS{Duan:2010,
    author = {Ganglong Duan AND Zhiwen Huang AND Jianren Wang},
    title = {Extreme Learning Machine for Financial Distress Prediction for
        Listed Company},
    booktitle = {2010 International Conference on Logistics Systems and
        Intelligent Management},
    year = {2010},
    month = {1},
    volume = {3},
    pages = {1961--1965},
    publisher = {IEEE Computer Society},
    abstract = {To overcome the shortages of the existing financial prediction
        models such as strict hypothesis, poor generalization ability, low
        prediction accuracy and low learning rate etc., a new early warning
        model of financial crisis have established for listed company using
        Extreme Learning Machine. From five dimensions of solvency,
        operating-ability, profitability, cash-ability and grow-ability, fifteen
        financial indexes were selected as the input variables; and the output
        variable was defined as whether the listed company had been special
        treated or not. The empirical analysis results show the training and
        validation accuracy of the model are 100\% and 92\% respectively, which
        concludes that learning and generalization abilities of this model are
        excellent, and which can meet the requirements of financial distress
        prediction for listed company.},
    doi = {10.1109/ICLSIM.2010.5461268},
    isbn = {978-1-4244-7331-1},
    keywords = {Extreme Learning Machine, Data Mining, Financial Crisis,
        Early-warning Mode},
    urldate = {2012-05-01}
}

% file = {Financial Data Mining based on Support Vector Machines and Ensemble Learning.pdf}
@INPROCEEDINGS{Lei:2010,
    author = {Shi Lei AND Ma Xinming AND Xi Lei AND Hu Xiaohong},
    title = {Financial Data Mining based on Support Vector Machines and Ensemble
        Learning},
    booktitle = {Proceedings of the 2010 International Conference on Intelligent
        Computation Technology and Automation},
    volume = {2},
    series = {ICICTA 2010},
    pages = {313--314},
    year = {2010},
    month = {05},
    publisher = {IEEE Computer Society},
    location = {Washington, DC, USA},
    abstract = {With the rapid development of e-commerce, financial data mining
        has been one of the most important research topics in the data mining
        community. Support vector machines (SVMs) and ensemble learning are two
        popular techniques in the machine learning field. In this paper, support
        vector machines and ensemble learning are used to classify financial
        data respectively. The experiments conducted on the public dataset show
        that compared with SVMs, ensemble learning achieves obvious improvement
        of performance.},
    doi = {10.1109/ICICTA.2010.787},
    isbn = {978-0-7695-4077-1},
    keywords = {Financial Data Mining, Support Vector Machines, Ensemble
        Learning}
}

% file = {Financial Forecasting - Advanced Machine Learning Techniques in Stock Market Analysis.pdf}
@INPROCEEDINGS{Yoo:2005,
    author = {Paul D. Yoo AND Maria H. Kim AND Tony Jan},
    title = {Financial Forecasting: Advanced Machine Learning Techniques in
        Stock Market Analysis},
    booktitle = {9th International Multitopic Conference},
    year = {2005},
    month = {12},
    pages = {1--7},
    organization = {IEEE},
    abstract = {The prediction of stock market has been an important issue in
        the field of finance, mathematics and engineering due to its great
        potential financial gain. In addition, uncertainty in the prediction of
        the financial time series has attracted interest from many researchers.
        In this study, we present recent developments in stock market prediction
        models, and discuss their strengths and limitations. In addition, we
        investigate diverse macroeconomic factors and their issues in the
        prediction of stock market. From this study, we found that incorporating
        event information into the prediction models plays important roles for
        more accurate prediction. Hence, an accurate event weighting method and
        a stable automated event extraction system are required for more
        accurate and reliable stock market prediction.},
    doi = {10.1109/INMIC.2005.334420},
    isbn = {0-7803-9429-1},
}

% file = {Low-latency FPGA Based Financial Data Feed Handler.pdf}
% TODO
@INPROCEEDINGS{Coyne:2011,
    author = {Robin Pottathuparambil AND Jack Coyne AND Jeffrey Allred AND
        William Lynch AND Vincent Natoli},
    title = {Low-latency FPGA Based Financial Data Feed Handler},
    maintitle = {IEEE International Symposium on Field-Programmable Custom
        Computing Machines},
    booktitle = {Proceedings of the 2011 IEEE 19th Annual International
        Symposium on Field-Programmable Custom Computing Machines},
    year = {2011},
    month = {05},
    series = {FCCM 2011},
    pages = {93--96},
    publisher = {IEEE Computer Society},
    location = {Washington, DC, USA},
    abstract = {Financial exchanges provide real time data feeds containing
        trade, order and status information to brokers, traders and other market
        makers. ITCH is one such market data feed that is disseminated by the
        NASDAQ exchange. The work presented in this paper describes an FPGA
        based ITCH feed handler and processing system. The handler, built on the
        Stone Ridge RDX-11 hardware platform with a combination of HDL and
        Impulse C, accepts and processes ITCH packet sat line speed with
        extremely low latency. Our implementation parses sixteen different stock
        symbols in the feed and generates an outbound packet on the NASDAQ one
        second heartbeat. The unit was tested with an artificial feed that could
        be adjusted in speed to simulate market surges. The system demonstrated
        a turnaround latency of $2.7 \mu{}s$ with very little variation for all
        tested feed rates. The CPU equivalent demonstrated $38 \pm 22 \mu{}s$
        (1x rate) with a long tail illustrating the variability inherent to
        processing by the host O/S. The FPGA solution demonstrated ultra-low,
        deterministic latency and was able to continue processing data at the
        line rate limit.},
    doi = {10.1109/FCCM.2011.50},
    isbn = {978-0-7695-4301-7},
    keywords = {FPGA, Low-latency Trading, ITCH feed handler}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% dir = {002. Chawla}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% file = {Density-preserving projections for large-scale local anomaly detection.pdf}
% TODO
@ARTICLE{Vries:2011,
    author = {Timothy de Vries AND Sanjay Chawla AND Michael E. Houle},
    title = {Density-Preserving Projections for Large-Scale Local Anomaly
        Detection},
    journaltitle = {Knowledge and Information Systems},
    year = {2011},
    month = {06},
    abstract = {Outlier or anomaly detection is a fundamental data mining task
        with the aim to identify data points, events, transactions which deviate
        from the norm. The identification of outliers in data can provide
        insights about the underlying data generating process. In general,
        outliers can be of two kinds: global and local. Global outliers are
        distinct with respect to the whole data set, while local outliers are
        distinct with respect to data points in their local neighbourhood. While
        several approaches have been proposed to scale up the process of global
        outlier discovery in large databases, this has not been the case for
        local outliers. We tackle this problem by optimising the use of local
        outlier factor (LOF) for large and high-dimensional data. We propose
        projection-indexed nearest-neighbours (PINN), a novel technique that
        exploits extended nearest-neighbour sets in a reduced-dimensional space
        to create an accurate approximation for k-nearest-neighbour distances,
        which is used as the core density measurement within LOF. The reduced
        dimensionality allows for efficient sub-quadratic indexing in the number
        of items in the data set, where previously only quadratic performance
        was possible. A detailed theoretical analysis of random projection (RP)
        and PINN shows that we are able to preserve the density of the intrinsic
        manifold of the data set after projection. Experimental results show
        that PINN outperforms the standard projection methods RP and PCA when
        measuring LOF for many high-dimensional real-world data sets of up to
        300,000 elements and 102,600 dimensions. A further investigation into
        the use of high-dimensionality-specific indexing such as spatial
        approximate sample hierarchy (SASH) shows that our novel technique holds
        benefits over even these types of highly efficient indexing. We cement
        the practical applications of our novel technique with insights into
        what it means to find local outliers in real data including image and
        text data, and include potential applications for this knowledge.},
    doi = {10.1007/s10115-011-0430-4},
    keywords = {anomaly detection, dimensionality reduction}
}

% file = {devries_icdm10.pdf}
% TODO
@INPROCEEDINGS{Vries:2010,
    author = {Timothy de Vries AND Sanjay Chawla AND Michael Houle},
    title = {Finding Local Anomalies in Very High Dimensional Space},
    booktitle = {10th IEEE International Conference on Data Mining},
    year = {2010},
    pages = {128--137},
    doi = {10.1109/ICDM.2010.151},
}

% file = {icdm04.pdf}
% TODO
@INPROCEEDINGS{Sun:2004,
    author = {Pei Sun AND Sanjay Chawla},
    title = {On Local Spatial Outliers},
    booktitle = {ICDM},
    year = {2004},
    pages = {209--216},
    doi = {10.1109/ICDM.2004.10097}
}

% file = {Khoa_pakdd10.pdf}
% TODO
@INPROCEEDINGS{Khoa:2010,
    author = {Nguyen Lu Dang Khoa AND Sanjay Chawla},
    title = {Robust Outlier Detection Using Commute Time and Eigenspace
        Embedding},
    booktitle = {PAKDD},
    year = {2010},
    abstract = {We present a method to ﬁnd outliers using `commute distance'
        computed from a random walk on graph. Unlike Euclidean distance, commute
        distance between two nodes captures both the distance between them and
        their local neighborhood densities. Indeed commute distance is the
        Euclidean distance in the space spanned by eigenvectors of the graph
        Laplacian matrix. We show by analysis and experiments that using this
        measure, we can capture both global and local outliers eﬀectively with
        just a distance based method. Moreover, the method can detect outlying
        clusters which other traditional methods often fail to capture and also
        shows a high resistance to noise than local outlier detection method.
        Moreover, to avoid the $O(n^{3})$ direct computation of commute
        distance, a graph component sampling and an eigenspace approximation
        combined with pruning technique reduce the time to $O(n \log n)$ while
        preserving the outlier ranking.},
    doi = {10.1007/978-3-642-13672-6_41},
    keywords = {outlier detection, commute distance, eigenspace embedding,
        random walk, nearest neighbor graph},
    pages = {422--434},
    volume = {2}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% dir = {003. Khoa}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% file = {Large Scale Anomaly Detection and Clustering Using Random Walks.pdf}
% TODO
@PHDTHESIS{Khoa:2012,
    author = {Nguyen Lu Dang Khoa},
    title = {Large Scale Anomaly Detection and Clustering Using Random Walks},
    institution = {University of Sydney},
    year = {2012},
    month = {03},
    abstract = {Many data mining and machine learning tasks involve calculating
        `distances' between data objects. Euclidean distance is the most widely
        used metric. However, there are situations where the Euclidean distance
        or other traditional metrics such as Mahalanobis distance and graph
        geodesic distance are not suitable to use as a measure of distance.
        
        Commute time is a robust measure derived from a random walk on graphs.
        In this thesis, we present methods to use commute time as a distance
        measure for data mining tasks such as anomaly detection and clustering.
        However, the computation of commute time involves the eigen
        decomposition of the graph Laplacian and thus is impractical for large
        graphs. We also propose methods to efficiently and accurately compute
        commute time in batch and incremental fashions.},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% dir = {004. Speilman-Teng}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% file = {Nearly-Linear Time Algorithms for Preconditioning and Solving Symmetric, Diagonally Dominant Linear Systems.pdf}
% TODO
@ARTICLE{Spielman:2006,
    author = {Daniel A. Spielman AND Shang-Hua Teng},
    title = {Nearly-Linear Time Algorithms for Preconditioning and Solving
        Symmetric, Diagonally Dominant Linear Systems},
    journaltitle = {CoRR},
    volume = {abs/cs/0607105},
    year = {2006},
    month = {09},
    keywords = {Numerical Analysis, Data Structures and Algorithms},
    url = {http://arxiv.org/pdf/cs/0607105v4.pdf},
    urldate = {2012-05-01},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% dir = {005. Matrices and other}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% file = {Circulation-2000-Goldberger-e215-20.pdf}
@ARTICLE{Goldberger:2000,
    author = {Ary L. Goldberger AND Luis A. N. Amaral AND Leon Glass AND Jeffrey
        M. Hausdorff AND Plamen Ch. Ivanov AND Roger G. Mark AND Joseph E.
        Mietus AND George B. Moody AND Chung-Kang Peng AND H. Eugene Stanley},
    title = {{PhysioBank, PhysioToolkit, and PhysioNet}: Components of a new
        research resource for complex physiologic signals},
    journaltitle = {Circulation},
    year = {2000},
    volume = {101},
    number = {23},
    pages = {e215--e220},
    abstract = {The newly inaugurated Research Resource for Complex Physiologic
        Signals, which was created under the auspices of the National Center for
        Research Resources of the National Institutes of Health, is intended to
        stimulate current research and new investigations in the study of
        cardiovascular and other complex biomedical signals. The resource has 3
        interdependent components. PhysioBank is a large and growing archive of
        well-characterized digital recordings of physiological signals and
        related data for use by the biomedical research community. It currently
        includes databases of multiparameter cardiopulmonary, neural, and other
        biomedical signals from healthy subjects and from patients with a
        variety of conditions with major public health implications, including
        life-threatening arrhythmias, congestive heart failure, sleep apnea,
        neurological disorders, and aging. PhysioToolkit is a library of
        open-source software for physiological signal processing and analysis,
        the detection of physiologically significant events using both classic
        techniques and novel methods based on statistical physics and nonlinear
        dynamics, the interactive display and characterization of signals, the
        creation of new databases, the simulation of physiological and other
        signals, the quantitative evaluation and comparison of analysis methods,
        and the analysis of nonstationary processes. PhysioNet is an on-line
        forum for the dissemination and exchange of recorded biomedical signals
        and open-source software for analyzing them. It provides facilities for
        the cooperative analysis of data and the evaluation of proposed new
        algorithms. In addition to providing free electronic access to
        PhysioBank data and PhysioToolkit software via the World Wide Web
        (\url{http://www.physionet.org}), PhysioNet offers services and training
        via on-line tutorials to assist users with varying levels of
        expertise.},
    doi = {10.1161/01.CIR.101.23.e215},
    keywords = {aging, databases, death, sudden, electrophysiology, heart rate,
        nervous system, autonomic, nonlinear dynamics},
    url = {http://circ.ahajournals.org/content/101/23/e215.full},
    urldate = {2012-05-01},
}

% file = {CS267_ Notes for Lecture 23, April 9, 1999.pdf}
@MISC{Berkeley:1999,
    author = {University of California, Berkeley},
    title = {Graph Partitioning, Part 2},
    year = {1999},
    month = {04},
    note = {CS267 Lecture Notes},
    url = {http://www.cs.berkeley.edu/~demmel/cs267/lecture20/lecture20.html},
    urldate = {2012-05-01}
}

% file = {lect02-09.pdf}
@MISC{Spielman:2009a,
    author = {Daniel A. Spielman},
    title = {The Laplacian},
    year = {2009},
    month = {09},
    howpublished = {\url{http://www.cs.yale.edu/homes/spielman/561/lect02-09.pdf}},
    note = {Lecture notes},
    urldate = {2012-05-01}
}

% file = {pati.pdf}
@MISC{Pati:2011,
    author = {Sukanta Pati},
    title = {Laplacian Matrix of a Graph},
    year = {2011},
    howpublished = {\url{http://www.lix.polytechnique.fr/~schwander/resources/mig/slides/pati.pdf}},
    note = {Talk prepared for the Indo-French workshop, 2011},
    urldate = {2012-05-01},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% dir = {006. Markov Chains}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% file = {Chapter11.pdf}
@BOOK{Grinstead:1997,
    author = {Charles M. Grinstead AND J. Laurie Snell},
    title = {Introduction to Probability},
    publisher = {American Mathematical Society},
    year = {1997},
    edition = {2},
    pages = {405--470},
    abstract = {Text is designed for an introductory probability course at the
        university level for sophomores, juniors, and seniors in mathematics,
        physical and social sciences, engineering, and computer science. It
        presents a thorough treatment of ideas and techniques necessary for a
        firm understanding of the subject. The text is also recommended for use
        in discrete probability courses. The material is organized so that the
        discrete and continuous probability discussions are presented in a
        separate, but parallel, manner. This organization does not emphasize an
        overly rigorous or formal view of probabililty and therefore offers some
        strong pedagogical value. Hence, the discrete discussions can sometimes
        serve to motivate the more abstract continuous probability discussions.
        \textbf{Features:} Key ideas are developed in a somewhat leisurely
        style, providing a variety of interesting applications to probability
        and showing some nonintuitive ideas. Over 600 exercises provide the
        opportunity for practicing skills and developing a sound understanding
        of ideas. Numerous historical comments deal with the development of
        discrete probability. The text includes many computer programs that
        illustrate the algorithms or the methods of computation for important
        problems.},
    isbn = {978-0821807491},
    url = {http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/amsbook.mac.pdf},
    urldate = {2012-05-03}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% dir = {007. Random walks on graphs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% file = {10.1.1.71.2194.pdf}
% TODO
@ARTICLE{Fouss:2007,
    author = {Francois Fouss AND Alain Pirotte AND Jean-Michel Renders AND Marco
        Saerens},
    title = {Random-Walk Computation of Similarities between Nodes of a Graph
        with Application to Collaborative Recommendation},
    journaltitle = {IEEE Trans. on Knowl. and Data Eng.},
    year = {2007},
    month = {3},
    volume = {19},
    number = {3},
    pages = {355--369},
    abstract = {This work presents a new perspective on characterizing the
        similarity between elements of a database or, more generally, nodes of a
        weighted and undirected graph. It is based on a Markov-chain model of
        random walk through the database. More precisely, we compute quantities
        (the average commute time, the pseudoinverse of the Laplacian matrix of
        the graph, etc) that provide similarities between any pair of nodes,
        having the nice property of increasing when the number of paths
        connecting those elements increases and when the ``length'' of paths
        decreases. It turns out that the square root of the average commute time
        is a Euclidean distance and that the pseudoinverse of the Laplacian
        matrix is a kernel (its elements are inner products closely related to
        commute times). A procedure for computing the subspace projection of the
        node vectors of the graph that preserves as much variance as possible in
        terms of the commute-time distance --- a principal component analysis
        (PCA) of the graph --- is also introduced. This graph PCA provides a
        nice interpretation to the ``Fiedler vector'', widely used for graph
        partitioning. The model is evaluated on a collaborative-recommendation
        task where suggestions are made about which movies people should watch
        based upon what they watched in the past. Experimental results on the
        MovieLens database show that the Laplacian-based similarities perform
        well in comparison with other methods. The model, which nicely fits into
        the so-called ``statistical relational learning'' framework, could also
        be used to compute document or word similarities, and, more generally,
        it could be applied to machine-learning and pattern-recognition tasks
        involving a database.},
    location = {Piscataway, NJ, USA},
    doi = {10.1109/TKDE.2007.46},
    issn = {1041-4347},
    keywords = {Graph analysis, graph and database mining, collaborative
        recommendation, graph kernels, spectral clustering, Fiedler vector,
        proximity measures, statistical relational learning.},
    publisher = {IEEE Educational Activities Department}
}

% file = {IJAIT.pdf}
% TODO
@ARTICLE{Moonesinghe:2008,
    author = {H. D. K. Moonesinghe AND Pang-Ning Tan},
    title = {Outrank: a Graph-Based Outlier Detection Framework Using Random
        Walk},
    journaltitle = {International Journal on Artificial Intelligence Tools},
    year = {2008},
    volume = {17},
    number = {1},
    pages = {19--36},
    abstract = {This paper introduces a stochastic graph-based algorithm, called
        OutRank, for detecting outliers in data. We consider two approaches for
        constructing a graph representation of the data, based on the object
        similarity and number of shared neighbors between objects. The heart of
        this approach is the Markov chain model that is built upon this graph,
        which assigns an outlier score to each object. Using this framework, we
        show that our algorithm is more robust than the existing outlier
        detection schemes and can effectively address the inherent problems of
        such schemes. Empirical studies conducted on both real and synthetic
        data sets show that significant improvements in detection rate and false
        alarm rate are achieved using the proposed framework.},
    doi = {10.1142/S0218213008003753},
    keywords = {Outlier detection, random walk, Markov chain}
}

% file = {p574-chandra.pdf}
% TODO
@INPROCEEDINGS{Chandra:1989,
    author = {Ashok K. Chandra AND Prabhakar Raghavan AND Walter L. Ruzzot AND
        Roman Smolensky AND Prasoon Tiwari},
    editor = {},
    title = {The Electrical Resistance of a Graph Captures its Commute and Cover
        Times},
    booktitle = {Proceedings of the Twenty-First Annual ACM Symposium on Theory
        of Computing},
    year = {1989},
    series = {STOC 1989},
    pages = {574--586},
    publisher = {ACM},
    location = {New York, NY, USA},
    abstract = {View an $n$-vertex, $m$-edge undirected graph as an electrical
        network with unit resistors as edges. We extend known relations between
        random walks and electrical networks by showing that resistance in this
        network is intimately connected with the lengths of random walks on the
        graph. For example, the commute time between two vertices $s$ and $t$
        (the expected length of a random walk from $s$ to $t$ and back) is
        precisely characterized by the effective resistance $R_{st}$ between $s$
        and $t$: commute time = $2mR_{st}$. Additionally, the cover time (the
        expected length of a random walk visiting all vertices) is characterized
        by the maximum resistance $R$ in the graph to within a factor of $\log
        n$: $mR ≤ cover time ≤ O (mR \log n)$. For many graphs, the bounds on
        cover time obtained in this manner are better than those obtained from
        previous techniques such as the eigenvalues of the adjacency matrix. In
        particular, using this approach, we improve known bounds on cover times
        for various classes of graphs, including high-degree graphs, expanders,
        and multi-dimensional meshes. Moreover, resistance seems to provide an
        intuitively appealing and tractable approach to these problems.},
    doi = {10.1145/73007.73062},
    isbn = {0-89791-307-8},
    url = {http://doi.acm.org/10.1145/73007.73062},
    urldate = {2012-05-03},
}

% file = {RandomWalks.pdf}
% TODO
@INCOLLECTION{Lovasz:1996,
    author = {L. Lov\'asz},
    editor = {D. {Mikl\'os} AND V. T. {S\'os} AND T. {Sz\H{o}nyi}},
    title = {Random Walks on Graphs: A Survey},
    booktitle = {Combinatorics, Paul Erd\H{o}s is Eighty},
    year = {1996},
    volume = {2},
    pages = {353--398},
    publisher = {J\'anos Bolyai Mathematical Society},
    location = {Budapest},
    abstract = {Various aspects of the theory of random walks on graphs are
        surveyed. In particular, estimates on the important parameters of access
        time, commute time, cover time and mixing time are discussed Connections
        with the eigenvalues of graphs and with electrical networks and the use
        of these connections in the study of random walks is described We also
        sketch recent algorithmic applications of random walks, in particular to
        the problem of sampling.},
    keywords = {networks structure dynamics}
}

% file = {walks.pdf}
@BOOK{Doyle:2006,
    author = {Peter G. Doyle AND J. Laurie Snell},
    title = {Random Walks and Electric Networks},
    year = {2006},
    publisher = {Mathematical Association of America},
    location = {Washington, DC, USA},
    keywords = {networks structure, dynamics}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% dir = {008. Outlier detection}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% file = {8.34.237.pdf}
% TODO
@ARTICLE{Knorr:2000,
    author = {Edwin M. Knorr AND Raymond T. Ng AND Vladimir Tucakov},
    title = {Distance-based Outliers: Algorithms and Applications},
    journaltitle = {The VLDB Journal --- The International Journal on Very Large
        Data Bases},
    year = {2000},
    volume = {8},
    pages = {237--253},
    number = {3--4},
    month = {02},
    abstract = {This paper deals with finding outliers (exceptions) in large,
        multidimensional datasets. The identification of outliers can lead to
        the discovery of truly unexpected knowledge in areas such as electronic
        commerce, credit card fraud, and even the analysis of performance
        statistics of professional athletes. Existing methods that we have seen
        for finding outliers can only deal efficiently with two
        dimensions/attributes of a dataset. In this paper, we study the notion
        of DB (distance-based) outliers. Specifically, we show that
        \begin{inparaenum}[(i)] \item outlier detection can be done efficiently
        for large datasets, and for k-dimensional datasets with large values of
        $k$ (e.g., $k \ge 5$); and \item outlier detection is a meaningful and
        important knowledge discovery task \end{inparaenum}. First, we present
        two simple algorithms, both having a complexity of $O(k \: N^2)$, $k$
        being the dimensionality and $N$ being the number of objects in the
        dataset. These algorithms readily support datasets with many more than
        two attributes. Second, we present an optimized cell-based algorithm
        that has a complexity that is linear with respect to $N$, but
        exponential with respect to $k$. We provide experimental results
        indicating that this algorithm significantly outperforms the two simple
        algorithms for $k \leq 4$. Third, for datasets that are mainly
        disk-resident, we present another version of the cell-based algorithm
        that guarantees at most three passes over a dataset. Again, experimental
        results show that this algorithm is by far the best for $k \leq 4$.
        Finally, we discuss our work on three real-life applications, including
        one on spatio-temporal data (e.g., a video surveillance application), in
        order to confirm the relevance and broad applicability of DB outliers.},
    doi = {10.1007/s007780050006}
}

% file = {Anomaly Detection.pdf}
% TODO
@ARTICLE{Chandola:2007,
    institution = {University of Minnesota},
    author = {Varun Chandola AND Arindam Banerjee AND Vipin Kumar},
    title = {Anomaly Detection: A Survey},
    journaltitle = {ACM Computing Serveys},
    year = {2009},
    month = {7},
    volume = {41},
    number = {3},
    pages = {15:1--15:58},
    abstract = {Anomaly detection is an important problem that has been
        researched within diverse research areas and application domains. Many
        anomaly detection techniques have been specifically developed for
        certain application domains, while others are more generic. This survey
        tries to provide a structured and comprehensive overview of the research
        on anomaly detection. We have grouped existing techniques into different
        categories based on the underlying approach adopted by each technique.
        For each category we have identified key assumptions, which are used by
        the techniques to differentiate between normal and anomalous behavior.
        When applying a given technique to a particular domain, these assumptions
        can be used as guidelines to assess the effectiveness of the technique
        in that domain. For each category, we provide a basic anomaly detection
        technique, and then show how the different existing techniques in that
        category are variants of the basic technique. This template provides an
        easier and succinct understanding of the techniques belonging to each
        category. Further, for each category, we identify the advantages and
        disadvantages of the techniques in that category. We also provide a
        discussion on the computational complexity of the techniques since it is
        an important issue in real application domains. We hope that this survey
        will provide a better understanding of the different directions in which
        research has been done on this topic, and how techniques developed in 
        one area can be applied in domains for which they were not intended to 
        begin with.},
    publisher = {ACM},
    location = {New York, NY, USA},
    issn = {0360-0300},
    keywords = {anomaly detection, outlier detection, database applications,
        data mining, algorithms},
    url = {http://www.cs.umn.edu/tech_reports_upload/tr2007/07-017.pdf},
    urldate = {2012-05-01}
}

% file = {outlier.pdf}
% TODO
@BOOK{Maimon:2005,
    author = {Oded Z Maimon AND Lior Rokach},
    title = {Data Mining and Knowledge Discovery Handbook: A Complete Guide for
        Practitioners and Researchers},
    year = {2005},
    pages = {1--16},
    publisher = {Kluwer Academic Publishers},
    abstract = {Data Mining and Knowledge Discovery Handbook organizes all major
        concepts, theories, methodologies, trends, challenges and applications
        of data mining (DM) and knowledge discovery in databases (KDD) into a
        coherent and unified repository. This book first surveys, then provides
        comprehensive yet concise algorithmic descriptions of methods, including
        classic methods plus the extensions and novel methods developed
        recently. This volume concludes with in-depth descriptions of data
        mining applications in various interdisciplinary industries including
        finance, marketing, medicine, biology, engineering, telecommunications,
        software, and security. Data Mining and Knowledge Discovery Handbook is
        designed for research scientists and graduate-level students in computer
        science and engineering.},
    isbn = {9780387244358}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% dir = {009. Principal Component Analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% file = {Jolliffe I. Principal Component Analysis (2ed., Springer, 2002)(518s)_MVsa_.pdf}
% TODO
@BOOK{Jolliffe:2002,
    author = {I.T. Jolliffe},
    title = {Principal Component Analysis},
    publisher = {Springer},
    year = {2002},
    edition = {2},
    isbn = {978-0387954424}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% dir = {010. Reconfigurable Computing}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% 00486081.pdf
% TODO
@INPROCEEDINGS{Vuillemin:2002,
    author = {Jean E. Vuillemin AND Patrice Bertin AND Didier Roncin AND Mark
        Shand AND Herv{\'e} H. Touati AND Philippe Boucard},
    editor = {Giovanni De Micheli AND Rolf Ernst AND Wayne Wolf},
    maintitle = {IEEE Transactions on VLSI Systems},
    booktitle = {Readings in Hardware/Software Co-Design},
    year = {2002},
    pages = {611--624},
    publisher = {Kluwer Academic Publishers},
    location = {Norwell, MA, USA},
    chapter = {Programmable active memories: reconfigurable systems come of age},
    isbn = {1-55860-702-1},
    keywords = {FPGA, PAM, field-programmable gate array, programmable active
        memory,    reconfigurable system}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% dir = {011. C for Matlab}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% file = {C-matlab.pdf}
@MISC{Uhl,
    author = {Andreas Uhl},
    title = {Calling C from Matlab: Introduction},
    note = {Presentation},
    url = {http://www.cosy.sbg.ac.at/~uhl/C-matlab.pdf},
    urldate = {2012-05-13}
}

% file = {cmex.pdf}
@MANUAL{Getreuer:2010,
    author = {Pascal Getreuer},
    title = {Writing MATLAB C/MEX Code},
    year = {2010},
    month = {04},
    pagetotal = {29},
    url = {http://www.mathworks.com/matlabcentral/fileexchange/authors/14582},
    urldate = {2012-05-13}
}

% file = {Examples of C source MEX files.pdf}
@MISC{MathWorks:1,
    author = {MathWorks},
    title = {Examples of C/C++ Source MEX-Files},
    note = {Product Documentation},
    url = {http://http://www.mathworks.com.au/help/techdoc/matlab_external/f12977.html},
    urldate = {2012-05-13}
}

% file = {MatlabCallsC.pdf}
@MISC{Kopecky,
    author = {Karen A. Kopecky},
    title = {Calling C and Fortran Programs from MATLAB},
    url = {http://www.karenkopecky.net/Teaching/Cclass/MatlabCallsC.pdf},
    urldate = {2012-05-13}
}

% file = {mex_file_programming.pdf}
@MISC{Hendriks:2011,
    author = {Cris L. Luengo Hendriks},
    title = {MEX-File Programming for Image Processing Using DIPimage},
    institution = {Delft University of Technology},
    year = {2011},
    month = {03}
}

% file = {MEX-files_Guide.pdf}
@MISC{MathWorks:2,
    author = {MathWorks},
    title = {MEX-files Guide},
    url = {http://www.mathworks.com.au/support/tech-notes/1600/1605.html},
    urldate = {2012-05-13}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% dir = {012. Pruning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% file = {10.1.76.9099.pdf}
% TODO
@INPROCEEDINGS{Bay:2003,
    author = {Stephen D. Bay AND Mark Schwabacher},
    title = {Mining Distance-Based Outliers in Near Linear Time with
        Randomization and a Simple Pruning Rule},
    booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on
        Knowledge Discovery and Data Mining},
    series = {KDD 2003},
    year = {2003},
    month = {08},
    isbn = {1-58113-737-0},
    location = {Washington, D.C.},
    pages = {29--38},
    numpages = {10},
    abstract = {Defining outliers by their distance to neighboring examples is a
        popular approach to finding unusual examples in a data set. Recently,
        much work has been conducted with the goal of finding fast algorithms
        for this task. We show that a simple nested loop algorithm that in the
        worst case is quadratic can give near linear time performance when the
        data is in random order and a simple pruning rule is used. We test our
        algorithm on real high-dimensional data sets with millions of examples
        and show that the near linear scaling holds over several orders of
        magnitude. Our average case analysis suggests that much of the
        efficiency is because the time to process non-outliers, which are the
        majority of examples, does not depend on the size of the data set.},
    doi = {10.1145/956750.956758},
    publisher = {ACM},
    keywords = {anomaly detection, diskbased algorithms, distance-based
        operations, outliers},
    url = {http://doi.acm.org/10.1145/956750.956758},
    urldate = {2012-05-21},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% dir = {013. Laplacian Matrices}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% file = {icm10post.pdf}
@INPROCEEDINGS{Spielman:2010,
    author = {Daniel A. Spielman},
    title = {Algorithms, Graph Theory, and Linear Equations in Laplacian
        Matrices},
    booktitle = {Proceedings of the International Congress of Mathematicians
        2010},
    year = {2010},
    abstract = {The Laplacian matrices of graphs are fundamental. In addition to
        facilitating the application of linear algebra to graph theory, they
        arise in many practical problems. In this talk we survey recent progress
        on the design of provably fast algorithms for solving linear equations
        in the Laplacian matrices of graphs. These algorithms motivate and rely
        upon fascinating primitives in graph theory, including low-stretch
        spanning trees, graph sparsiﬁers, ultra-sparsiﬁers, and local graph
        clustering. These are all connected by a definition of what it means for
        one graph to approximate another. While this deﬁnition is dictated by
        Numerical Linear Algebra, it proves useful and natural from a graph
        theoretic perspective.},
    pages = {2698--2722},
    doi = {10.1142/9789814324359.0164},
    url = {http://www.cs.yale.edu/homes/spielman/PAPERS/icm10post.pdf},
    urldate = {2012-05-23},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% dir = {014. AutoESL}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% file = {ug867-autoesl-ug.pdf}
% TODO
@MANUAL{XILINX:UserGuide,
    institution = {XILINX},
    title = {AutoESL User Guide},
    year = {2012},
    month = {04},
    numpages = {183}
}

% file = {ug868-autoesl-rg.pdf}
% TODO
@MANUAL{XILINX:ReferenceGuide,
    institution = {XILINX},
    title = {AutoESL Reference Guide},
    year = {2012},
    month = {04},
    numpages = {125}
}

% file = {ug870-autoesl-lg.pdf}
% TODO
@MANUAL{XILINX:CoreGuide,
    institution = {XILINX},
    title = {AutoESL Operator and Core Guide},
    year = {2012},
    month = {04},
    numpages = {18}
}

% file = {ug871-autoesl-intro.pdf}
% TODO
@MANUAL{XILINX:Tutorial,
    institution = {XILINX},
    title = {AutoESL Tutorial: Introduction},
    year = {2012},
    month = {04},
    numpages = {71}
}

% file = {ug889-autoesl-edk.pdf}
% TODO
@MANUAL{XILINX:Integrating,
    institution = {XILINX},
    title = {AutoESL Tutorial: Integrating with EDK},
    year = {2012},
    month = {04},
    numpages = {125}
}

% file = {ug923-autoesl-cs.pdf}
% TODO
@MANUAL{XILINX:CodingStyleGuide,
    institution = {XILINX},
    title = {AutoESL Coding Style Guide},
    year = {2012},
    month = {04},
    numpages = {160}
}

% file = {AutESL_Zynq_Training_Labs.pdf}
% TODO
@MANUAL{XILINX:AXITraining,
    institution = {XILINX},
    title = {AutoESL AXI Training: Lab Excercises},
    year = {2012},
    month = {05},
    numpages = {90},
    url = {http://www.xilinx.com/support/answers/50929.htm},
    urldate = {2012-09-08}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% dir = {015. General C}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% file = {Inline Functions in C.pdf}
@MISC{Kettlewell:1,
    author = {Richard Kettlewell},
    title = {Inline Functions in C},
    url = {http://http://www.greenend.org.uk/rjk/tech/inline.html},
    urldate = {2012-08-26}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% dir = {016. GNUplot}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% file = {gnuplot.pdf}
@MANUAL{gnuplot,
    author = {Hans-Bernhard Br\"{o}ker AND Ethan A Merritt and {many others}},
    title = {gnuplot 4.6: An Interactive Plotting Program},
    year = {2007},
    month = {08},
    url = {http://www.gnuplot.info}
}

% file = {gnuplotbook.pdf}
@BOOK{Janert:2009,
    author = {Philipp K. Janert},
    title = {Gnuplot in Action: Understanding Data with Graphs},
    publisher = {Manning Publications Co.},
    year = {2009},
    abstract = {Statistical data is only as valuable as your ability to analyze,
        interpret, and present it in a meaningful way. Gnuplot is the most
        widely used program to plot and visualize data for Unix/Linux systems
        and it is also popular for Windows and the Mac. It's open-source (as in
        free!), actively maintained, stable, and mature. It can deal with
        arbitrarily large data sets and is capable of producing high-quality,
        publication-ready graphics. So far, the only comprehensive documentation
        available about gnuplot is the online reference documentation, which
        makes it both hard to get started and almost impossible to get a
        complete overview over all of its features. If you've never tried
        gnuplot or have found it tough to get your arms around read on.
        Gnuplot in Action is the first comprehensive introduction to gnuplot
        from the basics to the power features and beyond. Besides providing a
        tutorial on gnuplot itself, it demonstrates how to apply and use gnuplot
        to extract intelligence from data. Particular attention is paid to
        tricky or poorly-explained areas. You will learn how to apply gnuplot to
        actual data analysis problems. This book looks at different types of
        graphs that can be generated with gnuplot and will discuss when and how
        to use them to extract actual information from data. One of gnuplot's
        main advantages is that it requires no programming skills nor knowledge
        of advanced mathematical or statistical concepts. Gnuplot in Action
        assumes you have no previous knowledge of either gnuplot or statistics
        and data analysis. The books starts out with basic gnuplot concepts,
        then describes in depth how to get a graph ready for final presentation
        and to make it look "just right" by including arrows, labels, and other
        decorations. Next the book looks at advanced concepts, such as
        multi-dimensional graphs and false-color plots "powerful features for
        special purposes. The author also describes advanced applications of
        gnuplot, such as how to script gnuplot so that it can run unattended as
        a batch job, and how to call gnuplot from within a CGI script to
        generate graphics for dynamic websites on demand. Gnuplot in Action
        makes gnuplot easy for anyone who needs to do data analysis, but doesn't
        have an education in analytical tools and methods. It's perfect for
        DBAs, programmers, and performance engineers; business analysts and
        MBAs; and Six-Sigma Black Belts and process engineers.},
    isbn = {1933988398, 9781933988399}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% dir = {017. Hardware Approach}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% file = {TODO}
@INPROCEEDINGS{Nelson:2010,
    author = {Jelani Nelson AND David P. Woodruff},
    title = {Fast Manhattan Sketches in Data Streams},
    booktitle = {Proceedings of the Twenty-Ninth ACM SIGMOD-SIGACT-SIGART
        Symposium on Principles of Database Systems},
    series = {PODS 2010},
    year = {2010},
    pages = {99--110},
    numpages = {12},
    abstract = {Several clustering methods have been developed for clustering
        network traffic in order to detect traffic anomalies and possible
        intrusions. Many of these methods are typically implemented in software.
        As a result they suffer performance limitations while processing real
        time traffic. This study presents a hardware implementation of the
        k-means clustering algorithm that is used to cluster network traffic.
        The implementation uses the Verilog hardware description language to
        build a circuit to read packet information from system memory and
        produce the output cluster assignments in a 32-bit register. After reset
        is applied, the circuit uses a state-machine that represents the k-means
        algorithm to process IP packets for a fixed number of iterations and
        then generates an interrupt to indicate that it had finished processing
        the data. The implementation is synthesized into a Field Programmable
        Gate Array in order to study the number of gates required for the
        implementation. The maximum achievable clock cycle without applying
        timing constraints is 40 MHz. To compare the performance of this
        implementation with a software-based implementation, a C version of the
        k-means algorithm is compiled, run and profiled with similar parameters
        of the hardware-based implementation. The results  show that the
        performance of the hardware-based implementation is approximately 300
        times faster than a software-based implementation.},
    publisher = {ACM},
    location = {Indianapolis, Indiana, USA},
    doi = {10.1145/1807085.1807101},
    isbn = {978-1-4503-0033-9},
    url = {http://www.cs.ucdavis.edu/~vemuri/papers/HardwareClustering-Rev7.pdf},
    urldate = {2012-09-07}
}

% file = {94.pdf}
% TODO

% file = {robnik03-mlj.pdf}
% TODO
@ARTICLE{Robnik-Sikonja:2003,
    author = {Marko Robnik-\v{S}ikonja AND Igor Kononenko},
    title = {Theoretical and Empirical Analysis of ReliefF and RReliefF},
    journal = {Mach. Learn.},
    year = {2003},
    month = {10},
    volume = {53},
    number = {1--2},
    pages = {23--69},
    numpages = {47},
    abstract = {Relief algorithms are general and successful attribute
        estimators. They are able to detect conditional dependencies between
        attributes and provide a unified view on the attribute estimation in
        regression and classification. In addition, their quality estimates have
        a natural interpretation. While they have commonly been viewed as
        feature subset selection methods that are applied in prepossessing step
        before a model is learned, they have actually been used successfully in
        a variety of settings, e.g., to select splits or to guide constructive
        induction in the building phase of decision or regression tree learning,
        as the attribute weighting method and also in the inductive logic
        programming.
        
        A broad spectrum of successful uses calls for especially careful
        investigation of variousfeatures Relief algorithms have. In this paper
        we theoretically and empirically investigate and discuss how and why
        they work, their theoretical and practical properties, their parameters,
        what kind of dependencies they detect, how do they scale up to large
        number of examples and features, how to sample data for them, how robust
        are they regarding the noise, how irrelevant and redundant attributes
        influence their output and how different metrics influences them.},
    publisher = {Kluwer Academic Publishers},
    location = {Hingham, MA, USA},
    doi = {10.1023/A:1025667309714},
    issn = {0885-6125},
    keywords = {Relief algorithm, attribute evaluation, classification, feature
        selection, regression},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% dir = {Books}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% file = {}
% TODO
@BOOK{Golub:1996,
    author = {Gene Howard Golub AND Charles F. Van Loan},
    title = {Matrix Computations},
    publisher = {Johns Hopkins University Press},
    year = {1996},
    edition = {3},
    abstract = {Revised and updated, the third edition of Golub and Van Loan's
        classic text in computer science provides essential information about
        the mathematical background and algorithmic skills required for the
        production of numerical software. This new edition includes thoroughly
        revised chapters on matrix multiplication problems and parallel matrix
        computations, expanded treatment of CS decomposition, an updated
        overview of floating point arithmetic, a more accurate rendition of the
        modified Gram-Schmidt process, and new material devoted to GMRES, QMR,
        and other methods designed to handle the sparse unsymmetric linear
        system problem.},
    isbn = {9780801854149}
}

% ebook = {}
% TODO
@BOOK{Hauck:2007,
    author = {Scott Hauck AND Andr\'{e} DeHon},
    title = {Reconfigurable Computing: The Theory and Practice of FPGA-Based
        Computation (Systems on Silicon)},
    publisher = {Morgan Kaufmann Publishers},
    year = {2007},
    edition = {1},
    abstract = {The main characteristic of Reconfigurable Computing is the
        presence of hardware that can be reconfigured to implement specific
        functionality more suitable for specially tailored hardware than on a
        simple uniprocessor. Reconfigurable computing systems join
        microprocessors and programmable hardware in order to take advantage of
        the combined strengths of hardware and software and have been used in
        applications ranging from embedded systems to high performance
        computing. Many of the fundamental theories have been identified and
        used by the Hardware/Software Co-Design research field. Although the
        same background ideas are shared in both areas, they have different
        goals and use different approaches. This book is intended as an
        introduction to the entire range of issues important to reconfigurable
        computing, using FPGAs as the context, or ``computing vehicles'' to
        implement this powerful technology. It will take a reader with a
        background in the basics of digital design and software programming and
        provide them with the knowledge needed to be an effective designer or
        researcher in this rapidly evolving field.},
    isbn = {978-0123705228}
}

% file = {}
@BOOK{Hawkins:1980,
    author = {Douglas M. Hawkins},
    title = {Identification of Outliers},
    series = {Monographs on Applied Probability and Statistics},
    year = {1980},
    publisher = {Chapman and Hall},
    isbn = {9780412219009}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Other
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% website = {http://archive.ics.uci.edu/ml}
@MISC{Frank:2010,
    author = {A. Frank AND A. Asuncion},
    year = {2010},
    title = {UCI Machine Learning Repository},
    url = {http://archive.ics.uci.edu/ml},
    institution = {University of California, Irvine, School of Information and Computer Sciences}
}
