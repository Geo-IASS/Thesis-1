%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The Function
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Function}
\label{profiling:function}
The function that was identified in the aforementioned tests as being most
significant to the execution time of the \algm{anomaly detection using commute
time} algorithm, and thus an ideal candidate for hardware acceleration, was the
\command{Top_N_Outlier_Pruning_Block} function. This function was originally
proposed by \citeauthor{Bay:2003} in \citetitle{Bay:2003}.

Essentially, this function simply performs a nested loops comparison of the
distances between pairwise vectors. Consequently, in the worst case, the
performance of this algorithm is very poor --- and it could require $O(n^2)$
distance computations and $O(\frac{n}{blocksize} \times n)$ data accesses
\cite{Bay:2003}. However, the use of randomization as well as pruning improves
the average-case complexity of the algorithm to be somewhat near-linear.

The main idea of the nested loop algorithm is that for each vector in the data
set, we keep track of the closest neighbours found so far. When an vector's
closest neighbours achieve a score lower than the cutoff, the data set is pruned
from the current block because it can no longer be an outlier. As the algorithm
proceeds to process more vectors, it is able to find more extreme outliers and
the cutoff increases along with pruning efficiency \cite{Bay:2003}. The score
function can be any monotonically decreasing function of the nearest neighbour
distances such as the distance to the $k$th nearest neighbour, or the average
distance to the k neighbours \cite{Bay:2003}.

In \citetitle{Bay:2003}, \citeauthor{Bay:2003} proves that the performance of
the \command{Top_N_Outlier_Pruning_Block} function was capable of scaling to
large, real, high-dimensional data sets, whilst maintaining a near linear
scaling performance. However, the algorithm depends on a number of assumptions,
violations of which can lead to poor performance.

\begin{enumerate}
    \item The algorithm assumes that the data is in random order. If the data is
        not in random order and is sorted then the performance can be poor.
    \item The outlier algorithm depends on the independence of examples. If
        examples are dependent in such a way that they have similar values (and
        will likely be in the set of $k$ nearest neighbours) this can cause
        performance to be poor as the algorithm may have to scan the entire data
        set to find the dependent examples.
    \item The third situation when our algorithm can perform poorly occurs when
        the data does not contain outliers. For example, our experiment with the
        examples drawn from a uniform distribution had very poor scaling.
        However, we believe data sets of this type are likely to be rare as most
        physical quantities one can measure have distributions with tails.
\end{enumerate}

\begin{algorithm}
    \input{chapter04/top-n-outlier-pruning-block.algm}
    \caption{TopN\_Outlier\_Pruning\_Block}
    \label{algm:TopNOutlierPruningBlock}
\end{algorithm}

The \command{Top_N_Outlier_Pruning_Block} function is listed in
\autoref{algm:TopNOutlierPruningBlock}. It is worthwhile to note that the
\command{score} function can be any monotonically decreasing function of the
nearest neighbour distances such as the distance to the $k$th nearest neighbour,
or the average distance to the $k$ neighbours \cite{Bay:2003}.

The main idea of the \command{Top_N_Outlier_Pruning_Block} algorithm is that for
each example in the input data set, the algorithm keeps track of the closest
neighbours found so far. When an example's closest neighbours achieve a score
lower than the `cufoff', the example is discarded because it can no longer be an
outlier. As more examples are processed, the algorithm finds more extreme
outliers and the cutoff increases along with pruning efficiency \cite{Bay:2003}.

In the worst case, the performance of the algorithm is very poor --- due to the
nested loops, it could require $O(n^2)$ distance computations and
$O(\frac{n}{blocksize} \times n)$ data accesses. However, \citeauthor{Bay:2003}
proved (through application of the algorithm to both real and synthetic data
sets) that the algorithm performs considerably better than the expected
$O(n^2)$ running time in the average case. The performance improvements over
similar algorithms were attributed to the application of randomization and
pruning techniques. The outlier pruning problem can be considered similar to the
problem of conducting a set of independent Bernoulli trials in which examples
are analysed until $k$ examples within distance $d$ are found, or until the data
set is exhausted. \citeauthor{Bay:2003} proved that the number of trials
expected to achieve $k$ examples within distance $d$ is given by:
\begin{equation}
    E[Y] \leq \frac{k}{\pi(\textbf{x})} + \Bigg(1-\sum_{y=k}^{N} P(Y=y)\Bigg) \times N
    \label{eqn:outlierPruneComplexity}
\end{equation}
Where $\pi(x)$ is the probability that a random drawn example lies within
distance $d$ of point $x$ and $P(Y=y)$ is the probability of obtaining the $k$th
success on trial $y$.

The first term in \autoref{eqn:outlierPruneComplexity} represents the number of
distance computations to eliminate non-outliers, and does not depend on $n$. The
second term represents the expected cost of outliers, and does depend on $n$,
yielding an overall quadratic dependency to process $n$ examples in total.
However, note that we typically set the program parameters to return a small an
possibly fixed number of outliers. Thus the first term dominates and we obtain
near linear performance \cite{Bay:2003}. More specifically, it was determined
that the primary factor determining the scaling is how the cut-off changes as
$n$ increases.