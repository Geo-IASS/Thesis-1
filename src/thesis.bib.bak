% This file was created with JabRef 2.7b.
% Encoding: UTF-8

@ARTICLE{CHANDOLA:2007,
  author = {Varun Chandola AND Arindam Banerjee AND Vipin Kumar},
  title = {Anomaly Detection: A Survey},
  journal = {ACM Computing Serveys},
  year = {2009},
  volume = {41(3)},
  number = {15},
  month = {August},
  abstract = {Anomaly detection is an important problem that has been researched
	within diverse research areas and application domains. Many anomaly
	detection techniques have been specifically developed for certain
	application domains, while others are more generic. This survey tries
	to provide a structured and comprehensive overview of the research
	on anomaly detection. We have grouped existing techniques into different
	categories based on the underlying approach adopted by each technique.
	For each category we have identified key assumptions, which are used
	by the techniques to differentiate between normal and anomalous behavior.
	When applying a given technique to a particular domain, these assumptions
	can be used as guidelines to assess the effectiveness of the technique
	in that domain. For each category, we provide a basic anomaly detection
	technique, and then show how the different existing techniques in
	that category are variants of the basic technique. This template
	provides an easier and succinct understanding of the techniques belonging
	to each category. Further, for each category, we identify the advantages
	and disadvantages of the techniques in that category. We also provide
	a discussion on the computational complexity of the techniques since
	it is an important issue in real application domains. We hope that
	this survey will provide a better understanding of the different
	directions in which research has been done on this topic, and how
	techniques developed in one area can be applied in domains for which
	they were not intended to begin with.},
  address = {4-192 EECS Building, 200 Union Street SE, Minneapolis MN 55455-0159,
	USA},
  file = {:/home/joshua/University/ELEC4712/Research/001. Initial Research/Anomaly Detection.pdf:PDF},
  institution = {University of Minnesota},
  keywords = {Anomaly Detection, Outlier Detection, Database Applications, Data
	Mining, Algorithms},
  timestamp = {2012.05.01},
  url = {http://www.cs.umn.edu/tech_reports_upload/tr2007/07-017.pdf}
}

@CONFERENCE{Duan:2010,
  author = {Ganglong Duan AND Zhiwen Huang AND Jianren Wang},
  title = {Extreme Learning Machine for Financial Distress Prediction for Listed
	Company},
  booktitle = {Logistics Systems and Intelligent Management},
  year = {2010},
  volume = {3},
  pages = {1961 - 1965},
  month = {January},
  organization = {IEEE},
  abstract = {To overcome the shortages of the existing financial prediction models
	such as strict hypothesis, poor generalization ability, low prediction
	accuracy and low learning rate etc., a new early warning model of
	financial crisis have established for listed company using Extreme
	Learning Machine. From five dimensions of solvency, operating-ability,
	profitability, cash-ability and grow-ability, fifteen financial indexes
	were selected as the input variables; and the output variable was
	defined as whether the listed company had been special treated or
	not. The empirical analysis results show the training and validation
	accuracy of the model are 100% and 92% respectively, which concludes
	that learning and generalization abilities of this model are excellent,
	and which can meet the requirements of financial distress prediction
	for listed company.},
  doi = {10.1109/ICLSIM.2010.5461268},
  file = {:/home/joshua/University/ELEC4712/Research/001. Initial Research/Extreme Learning Machine for Financial Distress Prediction for Listed Company.pdf:PDF},
  keywords = {Extreme Learning Machine, Data Mining, Financial Crisis, Early-warning
	Mode},
  timestamp = {2012.05.01}
}

@ARTICLE{Goldberger:2000,
  author = {Ary L. Goldberger AND Luis A. N. Amaral AND Leon Glass AND Jeffrey
	M. Hausdorff AND Plamen Ch. Ivanov AND Roger G. Mark AND Joseph E.
	Mietus AND George B. Moody AND Chung-Kang Peng AND H. Eugene Stanley},
  title = {PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research
	resource for complex physiologic signals},
  journal = {Circulation},
  year = {2000},
  pages = {215-220},
  number = {101},
  abstract = {The newly inaugurated Research Resource for Complex Physiologic Signals,
	which was created under the auspices of the National Center for Research
	Resources of the National Institutes of Health, is intended to stimulate
	current research and new investigations in the study of cardiovascular
	and other complex biomedical signals. The resource has 3 interdependent
	components. PhysioBank is a large and growing archive of well-characterized
	digital recordings of physiological signals and related data for
	use by the biomedical research community. It currently includes databases
	of multiparameter cardiopulmonary, neural, and other biomedical signals
	from healthy subjects and from patients with a variety of conditions
	with major public health implications, including life-threatening
	arrhythmias, congestive heart failure, sleep apnea, neurological
	disorders, and aging. PhysioToolkit is a library of open-source software
	for physiological signal processing and analysis, the detection of
	physiologically significant events using both classic techniques
	and novel methods based on statistical physics and nonlinear dynamics,
	the interactive display and characterization of signals, the creation
	of new databases, the simulation of physiological and other signals,
	the quantitative evaluation and comparison of analysis methods, and
	the analysis of nonstationary processes. PhysioNet is an on-line
	forum for the dissemination and exchange of recorded biomedical signals
	and open-source software for analyzing them. It provides facilities
	for the cooperative analysis of data and the evaluation of proposed
	new algorithms. In addition to providing free electronic access to
	PhysioBank data and PhysioToolkit software via the World Wide Web
	(http://www.physionet.org), PhysioNet offers services and training
	via on-line tutorials to assist users with varying levels of expertise.},
  doi = {10.1161/01.CIR.101.23.e215},
  file = {:/home/joshua/University/ELEC4712/Research/005. Matrices/Circulation-2000-Goldberger-e215-20.pdf:PDF},
  keywords = {aging, databases, death, sudden, electrophysiology, heart rate, nervous
	system, autonomic, nonlinear dynamics},
  timestamp = {2012.05.01},
  url = {http://circ.ahajournals.org/content/101/23/e215.full}
}

@BOOK{Golub1996,
  title = {Matrix Computations},
  publisher = {Johns Hopkins University Press},
  year = {1996},
  author = {Gene Howard Golub AND Charles F. Van Loan},
  edition = {3},
  month = {October},
  abstract = {Revised and updated, the third edition of Golub and Van Loan's classic
	text in computer science provides essential information about the
	mathematical background and algorithmic skills required for the production
	of numerical software. This new edition includes thoroughly revised
	chapters on matrix multiplication problems and parallel matrix computations,
	expanded treatment of CS decomposition, an updated overview of floating
	point arithmetic, a more accurate rendition of the modified Gram-Schmidt
	process, and new material devoted to GMRES, QMR, and other methods
	designed to handle the sparse unsymmetric linear system problem.},
  comment = {ISBN 9780801854149},
  timestamp = {2012.05.01}
}

@BOOK{Hauck:2007,
  title = {Reconfigurable Computing: The Theory and Practice of FPGA-Based Computation
	(Systems on Silicon)},
  publisher = {Morgan Kaufmann Publishers},
  year = {2007},
  author = {Scott Hauck AND André DeHon},
  edition = {1},
  month = {November},
  abstract = {The main characteristic of Reconfigurable Computing is the presence
	of hardware that can be reconfigured to implement specific functionality
	more suitable for specially tailored hardware than on a simple uniprocessor.
	Reconfigurable computing systems join microprocessors and programmable
	hardware in order to take advantage of the combined strengths of
	hardware and software and have been used in applications ranging
	from embedded systems to high performance computing. Many of the
	fundamental theories have been identified and used by the Hardware/Software
	Co-Design research field. Although the same background ideas are
	shared in both areas, they have different goals and use different
	approaches.This book is intended as an introduction to the entire
	range of issues important to reconfigurable computing, using FPGAs
	as the context, or "computing vehicles" to implement this powerful
	technology. It will take a reader with a background in the basics
	of digital design and software programming and provide them with
	the knowledge needed to be an effective designer or researcher in
	this rapidly evolving field.},
  comment = {ISBN 978-0123705228},
  timestamp = {2012.05.01}
}

@PHDTHESIS{Khoa:2012,
  author = {Nguyen Lu Dang Khoa},
  title = {Large Scale Anomaly Detection and Clustering Using Random Walks},
  school = {University of Sydney},
  year = {2012},
  month = {March},
  abstract = {Many data mining and machine learning tasks involve calculating ‘distances’
	between data objects. Euclidean distance is the most widely used
	metric. However, there are situations where the Euclidean distance
	or other traditional metrics such as Mahalanobis distance and graph
	geodesic distance are not suitable to use as a measure of distance.
	
	
	Commute time is a robust measure derived from a random walk on graphs.
	In this thesis, we present methods to use commute time as a distance
	measure for data mining tasks such as anomaly detection and clustering.
	However, the computation of commute time involves the eigen decomposition
	of the graph Laplacian and thus is impractical for large graphs.
	We also propose methods to efficiently and accurately compute commute
	time in batch and incremental fashions.},
  file = {:/home/joshua/University/ELEC4712/Research/003. Khoa/Large Scale Anomaly Detection and Clustering Using Random Walks.pdf:PDF},
  timestamp = {2012.05.01}
}

@ARTICLE{Knorr2000rohtua,
  author = {Edwin M. Knorr AND Raymond T. Ng AND Vladimir Tucakov},
  title = {Distance-based outliers: algorithms and applications},
  journal = {The VLDB Journal — The International Journal on Very Large Data Bases},
  year = {2000},
  volume = {8},
  number = {3-4},
  month = {February},
  abstract = {This paper deals with finding outliers (exceptions) in large, multidimensional
	datasets. The identification of outliers can lead to the discovery
	of truly unexpected knowledge in areas such as electronic commerce,
	credit card fraud, and even the analysis of performance statistics
	of professional athletes. Existing methods that we have seen for
	finding outliers can only deal efficiently with two dimensions/attributes
	of a dataset. In this paper, we study the notion of DB (distance-based)
	outliers. Specifically, we show that (i) outlier detection can be
	done efficiently for large datasets, and for k-dimensional datasets
	with large values of k (e.g., $k \ge 5$); and (ii), outlier detection
	is a meaningful and important knowledge discovery task.First, we
	present two simple algorithms, both having a complexity of $O(k \:
	N^2)$, k being the dimensionality and N being the number of objects
	in the dataset. These algorithms readily support datasets with many
	more than two attributes. Second, we present an optimized cell-based
	algorithm that has a complexity that is linear with respect to N,
	but exponential with respect to k. We provide experimental results
	indicating that this algorithm significantly outperforms the two
	simple algorithms for $k \leq 4$. Third, for datasets that are mainly
	disk-resident, we present another version of the cell-based algorithm
	that guarantees at most three passes over a dataset. Again, experimental
	results show that this algorithm is by far the best for $k \leq 4$.
	Finally, we discuss our work on three real-life applications, including
	one on spatio-temporal data (e.g., a video surveillance application),
	in order to confirm the relevance and broad applicability of DB outliers.},
  doi = {10.1007/s007780050006},
  timestamp = {2012.05.01}
}

@CONFERENCE{Lei:2010,
  author = {Shi Lei AND Ma Xinming AND Xi Lei AND Hu Xiaohong},
  title = {Financial Data Mining based on Support Vector Machines and Ensemble
	Learning},
  booktitle = {2010 International Conference on Intelligent Computation Technology
	and Automation},
  year = {2010},
  volume = {2},
  pages = {313 - 314},
  month = {May},
  organization = {IEEE},
  abstract = {With the rapid development of e-commerce, financial data mining has
	been one of the most important research topics in the data mining
	community. Support vector machines (SVMs) and ensemble learning are
	two popular techniques in the machine learning field. In this paper,
	support vector machines and ensemble learning are used to classify
	financial data respectively. The experiments conducted on the public
	dataset show that compared with SVMs, ensemble learning achieves
	obvious improvement of performance.},
  doi = {10.1109/ICICTA.2010.787},
  file = {:/home/joshua/University/ELEC4712/Research/001. Initial Research/Financial Data Mining based on Support Vector Machines and Ensemble Learning.pdf:PDF},
  keywords = {financial data mining, support vector machine, ensemble learning},
  timestamp = {2012.05.01}
}

@CONFERENCE{Li:2007,
  author = {Xiuquan Li AND Zhidong Deng},
  title = {A Machine Learning Approach to Predict Turning Points for Chaotic
	Financial Time Series},
  booktitle = {19th IEEE International Conference on Tools with Artificial Intelligence},
  year = {2007},
  volume = {2},
  pages = {331 - 335},
  month = {October},
  organization = {IEEE},
  abstract = {In this paper, a novel approach to predict turning points for chaotic
	financial time series is proposed based on chaotic theory and machine
	learning. The nonlinear mapping between different data points in
	primitive time series is derived and proven. Our definition of turning
	points produces an event characterization function, which can transform
	the profile of time series to a measure. The RBF neural network is
	further used as a nonlinear modeler. We discuss the threshold selection
	and give a procedure for threshold estimation using out-of-sample
	validation. The proposed approach is applied to the prediction problem
	of two real-world financial time series. The experimental results
	validate the effectiveness of our new approach.},
  doi = {10.1109/ICTAI.2007.105},
  file = {:/home/joshua/University/ELEC4712/Research/001. Initial Research/A Machine Learning Approach to Predict Turning Points for Chaotic Financial Time Series.pdf:PDF},
  timestamp = {2012.05.01}
}

@CONFERENCE{Nakayama:2000,
  author = {Hirotaka Nakayama AND Kengo Yoshii},
  title = {Active Forgetting in Machine Learning and its Application to Financial
	Problems},
  booktitle = {Neural Networks},
  year = {2000},
  volume = {5},
  pages = {123 - 128},
  month = {July},
  organization = {IEEE},
  abstract = {One of main features in financial investment problems is that the
	situation changes very often over time. Under this circumstance,
	in particular, it has been observed that additional learning plays
	an effective role. However, since the rule for classification becomes
	more and more complex with only additional learning, some appropriate
	forgetting is also necessary. It seems natural that many data are
	forgotten as the time elapses. On the other hand, it is expected
	more effective to forget unnecessary data actively. In this paper,
	several methods for active forgetting are suggested. The effectiveness
	of active forgetting is shown by examples in stock portfolio problems.},
  doi = {10.1109/IJCNN.2000.861445},
  file = {:/home/joshua/University/ELEC4712/Research/001. Initial Research/Active Forgetting in Machine Learning and its Application to Financial Problems.pdf:PDF},
  journal = {IEEE},
  keywords = {pattern classification, potential method, additional learning, forgetting},
  timestamp = {2012.05.01}
}

@MISC{Pati:2011,
  author = {Sukanta Pati},
  title = {Laplacian matrix of a graph},
  howpublished = {\url{http://www.lix.polytechnique.fr/~schwander/resources/mig/slides/pati.pdf}},
  year = {2011},
  note = {Talk prepared for the Indo-French workshop, 2011},
  file = {:/home/joshua/University/ELEC4712/Research/005. Matrices/pati.pdf:PDF},
  timestamp = {2012.05.01},
  url = {http://www.lix.polytechnique.fr/~schwander/resources/mig/slides/pati.pdf}
}

@CONFERENCE{Coyne:2011,
  author = {Robin Pottathuparambil AND Jack Coyne AND Jeffrey Allred AND William
	Lynch AND Vincent Natoli},
  title = {Low-latency FPGA Based Financial Data Feed Handler},
  booktitle = {IEEE International Symposium on Field-Programmable Custom Computing
	Machines},
  year = {2011},
  pages = {93 - 96},
  month = {May},
  organization = {IEEE},
  abstract = {Financial exchanges provide real time data feeds containing trade,
	order and status information to brokers, traders and other market
	makers. ITCH is one such market data feed that is disseminated by
	the NASDAQ exchange. The work presented in this paper describes an
	FPGA based ITCH feed handler and processing system. The handler,
	built on the Stone Ridge RDX-11 hardware platform with a combination
	of HDL and Impulse C, accepts and processes ITCH packet sat line
	speed with extremely low latency. Our implementation parses sixteen
	different stock symbols in the feed and generates an outbound packet
	on the NASDAQ one second heartbeat. The unit was tested with an artificial
	feed that could be adjusted in speed to simulate market surges. The
	system demonstrated a turnaround latency of 2.7μs with very little
	variation for all tested feed rates. The CPU equivalent demonstrated
	38±22μs(1x rate) with a long tail illustrating the variability inherent
	to processing by the host O/S. The FPGA solution demonstrated ultra-low,
	deterministic latency and was able to continue processing data at
	the line rate limit.},
  doi = {10.1109/FCCM.2011.50},
  file = {:/home/joshua/University/ELEC4712/Research/001. Initial Research/Low-latency FPGA Based Financial Data Feed Handler.pdf:PDF},
  journal = {IEEE International Symposium on Field-Programmable Custom Computing
	Machines},
  timestamp = {2012.05.01}
}

@MISC{Spielman:2009a,
  author = {Daniel A. Spielman},
  title = {The Laplacian},
  howpublished = {\url{http://www.cs.yale.edu/homes/spielman/561/lect02-09.pdf}},
  month = {September},
  year = {2009},
  note = {Lecture notes},
  file = {:/home/joshua/University/ELEC4712/Research/005. Matrices/lect02-09.pdf:PDF},
  timestamp = {2012.05.01},
  url = {http://www.cs.yale.edu/homes/spielman/561/lect02-09.pdf}
}

@ARTICLE{Spielman:2009,
  author = {Daniel A. Spielman AND Shang-Hua Teng},
  title = {Nearly-Linear Time Algorithms for Preconditioning and Solving Symmetric,
	Diagonally Dominant Linear Systems},
  year = {2009},
  month = {September},
  file = {:/home/joshua/University/ELEC4712/Research/004. Speilman-Teng/Nearly-Linear Time Algorithms for Preconditioning and Solving Symmetric, Diagonally Dominant Linear Systems.pdf:PDF},
  howpublished = {\url{http://arxiv.org/pdf/cs/0607105v4.pdf}},
  keywords = {Numerical Analysis, Data Structures and Algorithms},
  timestamp = {2012.05.01},
  url = {arXiv:cs/0607105v4}
}

@MISC{Berkeley:1999,
  author = {University of California, Berkeley},
  title = {Graph Partitioning, Part 2},
  howpublished = {\url{http://www.cs.berkeley.edu/~demmel/cs267/lecture20/lecture20.html}},
  month = {April},
  year = {1999},
  note = {CS267 Lecture Notes},
  timestamp = {2012.05.01},
  url = {http://www.cs.berkeley.edu/~demmel/cs267/lecture20/lecture20.html}
}

@ARTICLE{Vries:2011,
  author = {Timothy de Vries AND Sanjay Chawla AND Michael E. Houle},
  title = {Density-preserving projections for large-scale local anomaly detection},
  journal = {Knowledge and Information Systems},
  year = {2011},
  month = {June},
  abstract = {Outlier or anomaly detection is a fundamental data mining task with
	the aim to identify data points, events, transactions which deviate
	from the norm. The identification of outliers in data can provide
	insights about the underlying data generating process. In general,
	outliers can be of two kinds: global and local. Global outliers are
	distinct with respect to the whole data set, while local outliers
	are distinct with respect to data points in their local neighbourhood.
	While several approaches have been proposed to scale up the process
	of global outlier discovery in large databases, this has not been
	the case for local outliers. We tackle this problem by optimising
	the use of local outlier factor (LOF) for large and high-dimensional
	data. We propose projection-indexed nearest-neighbours (PINN), a
	novel technique that exploits extended nearest-neighbour sets in
	a reduced-dimensional space to create an accurate approximation for
	k-nearest-neighbour distances, which is used as the core density
	measurement within LOF. The reduced dimensionality allows for efficient
	sub-quadratic indexing in the number of items in the data set, where
	previously only quadratic performance was possible. A detailed theoretical
	analysis of random projection (RP) and PINN shows that we are able
	to preserve the density of the intrinsic manifold of the data set
	after projection. Experimental results show that PINN outperforms
	the standard projection methods RP and PCA when measuring LOF for
	many high-dimensional real-world data sets of up to 300,000 elements
	and 102,600 dimensions. A further investigation into the use of high-dimensionality-specific
	indexing such as spatial approximate sample hierarchy (SASH) shows
	that our novel technique holds benefits over even these types of
	highly efficient indexing. We cement the practical applications of
	our novel technique with insights into what it means to find local
	outliers in real data including image and text data, and include
	potential applications for this knowledge.},
  doi = {10.1007/s10115-011-0430-4},
  file = {:/home/joshua/University/ELEC4712/Research/002. Chawla/Density-preserving projections for large-scale local anomaly detection.pdf:PDF},
  keywords = {Anomaly detection, Dimensionality reduction},
  timestamp = {2012.05.01}
}

@CONFERENCE{Yoo:2005,
  author = {Paul D. Yoo AND Maria H. Kim AND Tony Jan},
  title = {Financial Forecasting: Advanced Machine Learning Techniques in Stock
	Market Analysis},
  booktitle = {9th International Multitopic Conference},
  year = {2005},
  pages = {1 - 7},
  month = {December},
  organization = {IEEE},
  abstract = {The prediction of stock market has been an important issue in the
	field of finance, mathematics and engineering due to its great potential
	financial gain. In addition, uncertainty in the prediction of the
	financial time series has attracted interest from many researchers.
	In this study, we present recent developments in stock market prediction
	models, and discuss their strengths and limitations. In addition,
	we investigate diverse macroeconomic factors and their issues in
	the prediction of stock market. From this study, we found that incorporating
	event information into the prediction models plays important roles
	for more accurate prediction. Hence, an accurate event weighting
	method and a stable automated event extraction system are required
	for more accurate and reliable stock market prediction.},
  doi = {10.1109/INMIC.2005.334420},
  file = {:/home/joshua/University/ELEC4712/Research/001. Initial Research/Financial Forecasting - Advanced Machine Learning Techniques in Stock Market Analysis.pdf:PDF},
  timestamp = {2012.05.01}
}

@CONFERENCE{Yu:2009,
  author = {Lean Yu AND Huanhuan Chen AND Shouyang Wang AND Kin Keung Lai},
  title = {Evolving Least Squares Support Vector Machines for Stock Market Trend
	Mining},
  booktitle = {IEEE Transactions of Evolutionary Computation},
  year = {2009},
  volume = {13},
  number = {1},
  pages = {87 - 102},
  month = {February},
  organization = {IEEE},
  abstract = {In this paper, an evolving least squares support vector machine (LSSVM)
	learning paradigm with a mixed kernel is proposed to explore stock
	market trends. In the proposed learning paradigm, a genetic algorithm
	(GA), one of the most popular evolutionary algorithms (EAs), is first
	used to select input features for LSSVM learning, i.e., evolution
	of input features. Then, another GA is used for parameters optimization
	of LSSVM, i.e., evolution of algorithmic parameters. Finally, the
	evolving LSSVM learning paradigm with best feature subset, optimal
	parameters, and a mixed kernel is used to predict stock market movement
	direction in terms of historical data series. For illustration and
	evaluation purposes, three important stock indices, S&P 500 Index,
	Dow Jones Industrial Average (DJIA) Index, and New York Stock Exchange
	(NYSE) Index, are used as testing targets. Experimental results obtained
	reveal that the proposed evolving LSSVM can produce some forecasting
	models that are easier to be interpreted by using a small number
	of predictive features and are more efficient than other parameter
	optimization methods. Furthermore, the produced forecasting model
	can significantly outperform other forecasting models listed in this
	paper in terms of the hit ratio. These findings imply that the proposed
	evolving LSSVM learning paradigm can be used as a promising approach
	to stock market tendency exploration.},
  doi = {10.1109/TEVC.2008.928176},
  file = {:/home/joshua/University/ELEC4712/Research/001. Initial Research/Evolving Least Squares Support Vector Machines for Stock Market Trend Mining.pdf:PDF},
  keywords = {Artificial neural networks (ANNs), evolutionary algorithms (EAs),
	feature selection, genetic algorithm (GA), least squares support
	vector machine (LSSVM), mixed kernel, parameter optimization, statistical
	models, stock market trend mining},
  timestamp = {2012.05.01}
}

